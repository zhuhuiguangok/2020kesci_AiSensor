{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "AISensor_deeplearning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "nLYIQiW-gom9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from scipy.signal import resample\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import os\n",
        "import numpy as np\n",
        "import datetime\n",
        "from tensorflow.keras.layers import Conv2D,Conv1D,Dense,Dropout,Input,GlobalMaxPooling2D,MaxPooling2D,MaxPooling1D,LayerNormalization,BatchNormalization,LSTM\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import regularizers\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "%matplotlib inline\n",
        "np.random.seed(2020)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLmT-yYwg4Lw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks\")\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGWMYisCyjoM",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "170f9f94-f322-45b0-de56-cfdb2d3e0ae6"
      },
      "source": [
        "#半监督1\n",
        "semi_test=pd.read_csv(\"data/semi_test_8_nobest_08-04-11-49_0.89783.csv\")\n",
        "semi_test.sort_values(by=[\"proba\"],ascending=False,inplace=True)\n",
        "semi_test_len=len(semi_test)\n",
        "print(semi_test_len)\n",
        "get_percentce=0.6\n",
        "semi_test_percent=semi_test.iloc[:round(get_percentce*semi_test_len)]\n",
        "sample_weight_train=semi_test_percent.proba.values\n",
        "print(len(semi_test_percent))\n",
        "test = pd.read_csv('data/sensor_test.csv')\n",
        "test=test.merge(semi_test_percent,how=\"inner\")\n",
        "print(test.isna().any())\n",
        "print(test.fragment_id.unique())\n",
        "test.fragment_id=test.fragment_id+7500\n",
        "test.drop(\"proba\",axis=1,inplace=True)\n",
        "print(len(test))\n",
        "train = pd.read_csv('data/sensor_train.csv')\n",
        "train_fragment_compare=train.fragment_id.unique()\n",
        "print(len(train))\n",
        "train=pd.concat([train,test])\n",
        "print(len(train))\n",
        "train_fragment_id=train.fragment_id.unique()\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7500\n",
            "4500\n",
            "fragment_id    False\n",
            "time_point     False\n",
            "acc_x          False\n",
            "acc_y          False\n",
            "acc_z          False\n",
            "acc_xg         False\n",
            "acc_yg         False\n",
            "acc_zg         False\n",
            "behavior_id    False\n",
            "proba          False\n",
            "dtype: bool\n",
            "[   0    1    4 ... 7496 7497 7498]\n",
            "257763\n",
            "425359\n",
            "683122\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJnN0PovBDX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_fragment_id=train.fragment_id.unique()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "--jGdy4fgonC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7640c3c7-f9fe-4cc7-85e1-1a2a3da1c99c"
      },
      "source": [
        "\n",
        "test = pd.read_csv('data/sensor_test.csv')\n",
        "sub = pd.read_csv('data/提交结果示例.csv')\n",
        "mapping = {0: 'A_0', 1: 'A_1', 2: 'A_2', 3: 'A_3', \n",
        "        4: 'D_4', 5: 'A_5', 6: 'B_1',7: 'B_5', \n",
        "        8: 'B_2', 9: 'B_3', 10: 'B_0', 11: 'A_6', \n",
        "        12: 'C_1', 13: 'C_3', 14: 'C_0', 15: 'B_6', \n",
        "        16: 'C_2', 17: 'C_5', 18: 'C_6'}\n",
        "mapping3 = {'A':0, 'B':1, 'C':2, 'D':2}\n",
        "\n",
        "### 处理标签数据\n",
        "train['action_type'] = train['behavior_id'].map(mapping)\n",
        "train['action_type_1'] = train['action_type'].map(lambda x:x.split('_')[0])\n",
        "train['action_type_2'] = train['action_type'].map(lambda x:int(x.split('_')[1]))\n",
        "train['sence'] = train['behavior_id'].map(lambda x:mapping3[mapping[x].split('_')[0]])\n",
        "train['action'] = train['behavior_id'].map(lambda x:int(mapping[x].split('_')[1]))\n",
        "y_cls = train.groupby('fragment_id')['behavior_id'].min()\n",
        "y_scene = train.groupby('fragment_id')['sence'].min()\n",
        "y_action = train.groupby('fragment_id')['action'].min()\n",
        "print(\"label\",len(y_cls))\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label 11792\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMcPGAnJ9LLZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "47691cf0-e009-4746-97a9-5abe07127306"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fragment_id</th>\n",
              "      <th>time_point</th>\n",
              "      <th>acc_x</th>\n",
              "      <th>acc_y</th>\n",
              "      <th>acc_z</th>\n",
              "      <th>acc_xg</th>\n",
              "      <th>acc_yg</th>\n",
              "      <th>acc_zg</th>\n",
              "      <th>behavior_id</th>\n",
              "      <th>action_type</th>\n",
              "      <th>action_type_1</th>\n",
              "      <th>action_type_2</th>\n",
              "      <th>sence</th>\n",
              "      <th>action</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>27</td>\n",
              "      <td>0.3</td>\n",
              "      <td>-0.3</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.6</td>\n",
              "      <td>4.5</td>\n",
              "      <td>8.8</td>\n",
              "      <td>0</td>\n",
              "      <td>A_0</td>\n",
              "      <td>A</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>108</td>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>0.4</td>\n",
              "      <td>4.7</td>\n",
              "      <td>8.4</td>\n",
              "      <td>0</td>\n",
              "      <td>A_0</td>\n",
              "      <td>A</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>198</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>0.9</td>\n",
              "      <td>4.6</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0</td>\n",
              "      <td>A_0</td>\n",
              "      <td>A</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>297</td>\n",
              "      <td>0.1</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.8</td>\n",
              "      <td>4.7</td>\n",
              "      <td>7.2</td>\n",
              "      <td>0</td>\n",
              "      <td>A_0</td>\n",
              "      <td>A</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>388</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.9</td>\n",
              "      <td>4.7</td>\n",
              "      <td>8.9</td>\n",
              "      <td>0</td>\n",
              "      <td>A_0</td>\n",
              "      <td>A</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   fragment_id  time_point  acc_x  ...  action_type_2  sence  action\n",
              "0            0          27    0.3  ...              0      0       0\n",
              "1            0         108    0.1  ...              0      0       0\n",
              "2            0         198    0.1  ...              0      0       0\n",
              "3            0         297    0.1  ...              0      0       0\n",
              "4            0         388    0.1  ...              0      0       0\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "F1fsosgIgonF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train['accmod'] = (train.acc_x ** 2 + train.acc_y ** 2 + train.acc_z ** 2) ** .5\n",
        "train['accmodg'] = (train.acc_xg ** 2 + train.acc_yg ** 2 + train.acc_zg ** 2) ** .5\n",
        "#train['accxy'] = (train['acc_x'] ** 2 + train['acc_y'] ** 2) ** 0.5\n",
        "#train['accyz'] = (train['acc_y'] ** 2 + train['acc_z'] ** 2) ** 0.5\n",
        "#train['accxz'] = (train['acc_x'] ** 2 + train['acc_z'] ** 2) ** 0.5\n",
        "#train['accxyg'] = (train['acc_xg'] ** 2 + train['acc_yg'] ** 2) ** 0.5\n",
        "#train['accyzg'] = (train['acc_yg'] ** 2 + train['acc_zg'] ** 2) ** 0.5\n",
        "#train['accxzg'] = (train['acc_xg'] ** 2 + train['acc_zg'] ** 2) ** 0.5\n",
        "\n",
        "test['accmod'] = (test.acc_x ** 2 + test.acc_y ** 2 + test.acc_z ** 2) ** .5\n",
        "test['accmodg'] = (test.acc_xg ** 2 + test.acc_yg ** 2 + test.acc_zg ** 2) ** .5                                        \n",
        "#test['accxy'] = (test['acc_x'] ** 2 + test['acc_y'] ** 2) ** 0.5\n",
        "#test['accyz'] = (test['acc_y'] ** 2 + test['acc_z'] ** 2) ** 0.5\n",
        "#test['accxz'] = (test['acc_x'] ** 2 + test['acc_z'] ** 2) ** 0.5\n",
        "#test['accxyg'] = (test['acc_xg'] ** 2 + test['acc_yg'] ** 2) ** 0.5\n",
        "#test['accyzg'] = (test['acc_yg'] ** 2 + test['acc_zg'] ** 2) ** 0.5\n",
        "#test['accxzg'] = (test['acc_xg'] ** 2 + test['acc_zg'] ** 2) ** 0.5\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "rXKu5xIkgonL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "e7e3d255-3564-49d2-8a3a-c1f8d794ed81"
      },
      "source": [
        "x = np.zeros((len(train_fragment_id), 60, 8))  \n",
        "t = np.zeros((7500, 60, 8))\n",
        "for i in tqdm(range(len(train_fragment_id))):\n",
        "    tmp = train[train.fragment_id ==train_fragment_id[i]][:60]\n",
        "    x[i,:,:] = resample(tmp.drop(['fragment_id', 'time_point', 'behavior_id','action_type','action_type_1','action_type_2',\"sence\",\"action\"],axis=1), 60, np.array(tmp.time_point))[0]\n",
        "for i in tqdm(range(7500)):\n",
        "    tmp = test[test.fragment_id == i][:60]\n",
        "    t[i,:,:] = resample(tmp.drop(['fragment_id', 'time_point'],axis=1), 60, np.array(tmp.time_point))[0]\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 11792/11792 [00:33<00:00, 351.88it/s]\n",
            "100%|██████████| 7500/7500 [00:19<00:00, 392.75it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EG4nnVTfGBUJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def jitter(x,snr_db):\n",
        "    \"\"\"\n",
        "    根据信噪比添加噪声\n",
        "    :param x:\n",
        "    :param snr_db:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # 随机选择信噪比\n",
        "    assert isinstance(snr_db, list)\n",
        "    snr_db_low = snr_db[0]\n",
        "    snr_db_up = snr_db[1]\n",
        "    snr_db = np.random.randint(snr_db_low, snr_db_up, (1,))[0]\n",
        "\n",
        "    snr = 10 ** (snr_db / 10)\n",
        "    Xp = np.sum(x ** 2, axis=0, keepdims=True) / x.shape[0]  # 计算信号功率\n",
        "    Np = Xp / snr  # 计算噪声功率\n",
        "    n = np.random.normal(size=x.shape, scale=np.sqrt(Np), loc=0.0)  # 计算噪声 loc均值，scale方差\n",
        "    xn = x + n\n",
        "    return xn\n",
        "\n",
        "def standardization(X):\n",
        "    #X batch_size 60 8\n",
        "    b,h,w=X.shape[0],X.shape[1],X.shape[2]\n",
        "    x1 = X.reshape(-1, X.shape[-1])\n",
        "    mu = np.mean(x1, axis=0)\n",
        "    sigma = np.std(x1, axis=0, ddof=1)\n",
        "    x1 = ((x1 - mu) / (sigma))\n",
        "    print(x1.shape)\n",
        "    X_=x1.reshape(b,h,w)\n",
        "    return X_"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naxudOKFGNMV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "a2d2e0a0-c394-4894-f6aa-dca346f4b7d2"
      },
      "source": [
        "#标准化\n",
        "x=standardization(x)\n",
        "print(x.shape)\n",
        "t=standardization(t)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(707520, 8)\n",
            "(11792, 60, 8)\n",
            "(450000, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ1ZCNip2kER",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Disout(tf.keras.layers.Layer):\n",
        "    '''\n",
        "    disout\n",
        "    论文：https://arxiv.org/abs/2002.11022\n",
        "    '''\n",
        "    def __init__(self, dist_prob, block_size=5, alpha=1, **kwargs):\n",
        "        super(Disout, self).__init__(**kwargs)\n",
        "        self.dist_prob = dist_prob\n",
        "        self.weight_behind=None\n",
        "\n",
        "        self.alpha = alpha\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        pass\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x):\n",
        "        '''x：(batch_size,h,w,c)'''\n",
        "        if not self.trainable:\n",
        "            return x\n",
        "        else:\n",
        "            if tf.math.equal(tf.rank(x),4):\n",
        "                x_shape = tf.shape(x)\n",
        "                x_size = x_shape[1:3]\n",
        "                x_size_f = tf.cast(x_size, tf.float32)\n",
        "                # 计算block_size\n",
        "                x_block_size_f = tf.constant((self.block_size, self.block_size), tf.float32)\n",
        "                # x_block_size_f = x_size_f * self.block_size\n",
        "                # x_block_size_f = tf.math.maximum(x_block_size_f, 1)\n",
        "                x_block_size = tf.cast(x_block_size_f, tf.int32)\n",
        "                # 根据dist_prob，计算block_num\n",
        "                x_block_num = (x_size_f[0] * x_size_f[1]) * self.dist_prob / (x_block_size_f[0] * x_block_size_f[1])\n",
        "                # 计算block在中心区域出现的概率\n",
        "                x_block_rate = x_block_num / ((x_size_f[0] - x_block_size_f[0] + 1) * (x_size_f[1] - x_block_size_f[1] + 1))\n",
        "                # tf.print('x_block_rate:', x_block_rate)\n",
        "                # 根据概率生成block区域\n",
        "                x_block_center = tf.random.uniform((x_shape[0], x_size[0] - x_block_size[0] + 1, x_size[1] - x_block_size[1] + 1, x_shape[3]), dtype=tf.float32)\n",
        "                x_block_padding_t = x_block_size[0] // 2\n",
        "                x_block_padding_b = x_size_f[0] - tf.cast(x_block_padding_t, tf.float32) - (x_size_f[0] - x_block_size_f[0] + 1.0)\n",
        "                x_block_padding_b = tf.cast(x_block_padding_b, tf.int32)\n",
        "                x_block_padding_l = x_block_size[1] // 2\n",
        "                x_block_padding_r = x_size_f[1] - tf.cast(x_block_padding_l, tf.float32) - (x_size_f[1] - x_block_size_f[1] + 1.0)\n",
        "                x_block_padding_r = tf.cast(x_block_padding_r, tf.int32)\n",
        "                x_block_padding = tf.pad(x_block_center,[[0, 0],[x_block_padding_t, x_block_padding_b],[x_block_padding_l, x_block_padding_r],[0, 0]])\n",
        "                x_block = tf.cast(x_block_padding<x_block_rate, tf.float32)\n",
        "                x_block = tf.nn.max_pool2d(x_block, ksize=[self.block_size, self.block_size], strides=[1, 1], padding='SAME')\n",
        "                # block百分比\n",
        "                # x_block_percent_ones = tf.reduce_sum(x_block) / tf.reduce_prod(tf.cast(tf.shape(x_block), tf.float32))\n",
        "                # tf.print('x_block_percent_ones:', x_block_percent_ones, tf.shape(x_block))\n",
        "                # 特征叠加\n",
        "                x_abs = tf.abs(x)\n",
        "                x_sum = tf.math.reduce_sum(x_abs, axis=-1, keepdims=True)\n",
        "                x_max = tf.math.reduce_max(x_sum, axis=(1, 2), keepdims=True)\n",
        "                x_max_c = tf.math.reduce_max(x_abs, axis=(1, 2), keepdims=True)\n",
        "                x_sum_c = tf.math.reduce_sum(x_max_c, axis=-1, keepdims=True)\n",
        "                x_v = x_sum / x_sum_c\n",
        "                # tf.print('x_v:', tf.shape(x_v), tf.math.reduce_min(x_v), tf.math.reduce_max(x_v))\n",
        "                # 特征方差\n",
        "                # x_variance = tf.math.reduce_variance(x_sum, axis=(1, 2), keepdims=True)\n",
        "                # tf.print('x_variance:', tf.shape(x_variance), tf.math.reduce_min(x_variance), tf.math.reduce_max(x_variance))\n",
        "                # 叠加扰动\n",
        "                x_max = tf.reduce_max(x, axis=(1,2), keepdims=True)\n",
        "                x_min = tf.reduce_min(x, axis=(1,2), keepdims=True)\n",
        "                x_block_random = tf.random.uniform(x_shape, dtype=x.dtype) * (x_max - x_min) + x_min\n",
        "                x_block_random = x_block_random * (self.alpha * x_v + 0.3) + x * (1.0 - self.alpha * x_v - 0.3)\n",
        "                x = x * (1-x_block) + x_block_random * x_block\n",
        "                return x\n",
        "            else:\n",
        "                return x\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        '''计算输出shape'''\n",
        "        return input_shape\n",
        "        \n",
        "class Disout1D(tf.keras.layers.Layer):\n",
        "    '''\n",
        "    disout\n",
        "    论文：https://arxiv.org/abs/2002.11022\n",
        "    '''\n",
        "\n",
        "    def __init__(self, dist_prob, block_size=5, alpha=0.5, **kwargs):\n",
        "        super(Disout1D, self).__init__(**kwargs)\n",
        "        self.dist_prob = dist_prob\n",
        "\n",
        "        self.alpha = alpha\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        pass\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x):\n",
        "        '''x：(batch_size,h,w,c)'''\n",
        "        if not self.trainable:\n",
        "            return x\n",
        "        else:\n",
        "            if tf.math.equal(tf.rank(x),2):\n",
        "                x_shape = tf.shape(x)\n",
        "                x_size = x_shape[1]\n",
        "                x_size_f = tf.cast(x_size, tf.float32)\n",
        "                # 计算block_size\n",
        "                x_block_size_f = tf.constant(self.block_size, tf.float32)\n",
        "                # x_block_size_f = x_size_f * self.block_size\n",
        "                # x_block_size_f = tf.math.maximum(x_block_size_f, 1)\n",
        "                x_block_size = tf.cast(x_block_size_f, tf.int32)\n",
        "                # 根据dist_prob，计算block_num\n",
        "                x_block_num = (x_size_f) * self.dist_prob / (x_block_size_f)\n",
        "                # 计算block在中心区域出现的概率\n",
        "                x_block_rate = x_block_num / ((x_size_f - x_block_size_f + 1))\n",
        "                # 根据概率生成block区域\n",
        "                x_block_center = tf.random.uniform((x_shape[0], x_size - x_block_size + 1), dtype=tf.float32)\n",
        "                x_block_padding_t = x_block_size // 2\n",
        "                x_block_padding_b = x_size_f - tf.cast(x_block_padding_t, tf.float32) - (x_size_f - x_block_size_f + 1.0)\n",
        "                x_block_padding_b = tf.cast(x_block_padding_b, tf.int32)\n",
        "                x_block_padding = tf.pad(x_block_center,[[0, 0],[x_block_padding_t, x_block_padding_b]])\n",
        "                x_block = tf.cast(x_block_padding<x_block_rate, tf.float32)\n",
        "                x_block = tf.expand_dims(x_block, axis=-1)\n",
        "                x_block = tf.nn.max_pool1d(x_block, ksize=[self.block_size], strides=[1], padding='SAME')\n",
        "                x_block = tf.reshape(x_block, x_shape)\n",
        "                # 叠加扰动\n",
        "                x_max = tf.reduce_max(x, axis=1, keepdims=True)\n",
        "                x_min = tf.reduce_min(x, axis=1, keepdims=True)\n",
        "                x_block_random = tf.random.uniform(x_shape, dtype=x.dtype) * (x_max - x_min) + x_min\n",
        "                x_block_random = x_block_random * (1.0 - self.alpha) + x * self.alpha\n",
        "                x = x * (1-x_block) + x_block_random * x_block\n",
        "                return x\n",
        "            else:\n",
        "                return x\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        '''计算输出shape'''\n",
        "        return input_shape\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMSq3nlU2Sel",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_multi(y_true, y_pred, smooth_wight=0.001, eps=0.001):\n",
        "    X_cls, X_sence, X_action = y_pred[0], y_pred[1], y_pred[2]\n",
        "    y_cls, y_sence, y_action = y_true[0], y_true[1], y_true[2]\n",
        "    print(X_cls.shape,X_sence.shape,X_action.shape)\n",
        "    print(y_cls.shape,y_cls.shape,y_cls.shape)\n",
        "    print(\"test\")\n",
        "    loss_cls = tf.losses.categorical_crossentropy(y_cls, X_cls)\n",
        "    loss_sence = tf.losses.categorical_crossentropy(y_sence, X_sence)\n",
        "    loss_action = tf.losses.categorical_crossentropy(y_action, X_action)\n",
        "    \n",
        "#     return [loss_cls, loss_sence, loss_action]\n",
        "    return tf.reduce_mean(1.0/7 * loss_sence + 1.0/3 * loss_action + 11/21 * loss_cls)\n",
        "\n",
        "def get_acc_combo():\n",
        "    def combo(y, y_pred):\n",
        "        # 数值ID与行为编码的对应关系\n",
        "        mapping = {0: 'A_0', 1: 'A_1', 2: 'A_2', 3: 'A_3',\n",
        "            4: 'D_4', 5: 'A_5', 6: 'B_1',7: 'B_5',\n",
        "            8: 'B_2', 9: 'B_3', 10: 'B_0', 11: 'A_6',\n",
        "            12: 'C_1', 13: 'C_3', 14: 'C_0', 15: 'B_6',\n",
        "            16: 'C_2', 17: 'C_5', 18: 'C_6'}\n",
        "        # 将行为ID转为编码\n",
        "        code_y, code_y_pred = mapping[int(y)], mapping[int(y_pred)]\n",
        "        if code_y == code_y_pred: #编码完全相同得分1.0\n",
        "            return 1.0\n",
        "        elif code_y.split(\"_\")[0] == code_y_pred.split(\"_\")[0]: #编码仅字母部分相同得分1.0/7\n",
        "            return 1.0/7\n",
        "        elif code_y.split(\"_\")[1] == code_y_pred.split(\"_\")[1]: #编码仅数字部分相同得分1.0/3\n",
        "            return 1.0/3\n",
        "        else:\n",
        "            return 0.0\n",
        "\n",
        "    confusionMatrix=np.zeros((19,19))\n",
        "    for i in range(19):\n",
        "        for j in range(19):\n",
        "            confusionMatrix[i,j]=combo(i,j)\n",
        "    confusionMatrix=tf.convert_to_tensor(confusionMatrix)\n",
        "\n",
        "    def acc_combo(y, y_pred):\n",
        "        y=tf.argmax(y,axis=1)\n",
        "        y_pred = tf.argmax(y_pred, axis=1)\n",
        "        indices=tf.stack([y,y_pred],axis=1)#在1轴增加一个维度\n",
        "        scores=tf.gather_nd(confusionMatrix,tf.cast(indices,tf.int32))\n",
        "        return tf.reduce_mean(scores)\n",
        "    return acc_combo"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "tysQMxqjgonR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kfold = StratifiedKFold(10, shuffle=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3dIM1gd_MLu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "def shuffle_index_fun(index):\n",
        "    random.shuffle(index)\n",
        "    return index.copy()\n",
        "\n",
        "def shuffle_data(data_x, data_y):\n",
        "    shuffle_index = list(range(data_x.shape[0]))\n",
        "    r_index = shuffle_index_fun(shuffle_index)\n",
        "    return data_x[r_index], data_y[r_index]\n",
        "\n",
        "def mixup(batch_x, batch_y, random_max=0.2):\n",
        "    shuffle_index = list(range(batch_x.shape[0]))\n",
        "    r_index1 = shuffle_index_fun(shuffle_index)\n",
        "    r_index2 = shuffle_index_fun(shuffle_index) \n",
        "    rd =np.random.beta(random_max, random_max)\n",
        "    batch_x_mixup = (1-rd) * batch_x[r_index1] + rd * batch_x[r_index2]\n",
        "    batch_y_mixup = (1-rd) * batch_y[r_index1] + rd * batch_y[r_index2]\n",
        "    return batch_x_mixup, batch_y_mixup\n",
        "\n",
        "def mixup_uniform(batch_x, batch_y, random_max=0.1):\n",
        "    shuffle_index = list(range(batch_x.shape[0]))\n",
        "    r_index1 = shuffle_index_fun(shuffle_index)\n",
        "    r_index2 = shuffle_index_fun(shuffle_index) \n",
        "    rd =np.random.uniform(0.05, random_max)\n",
        "    batch_x_mixup = (1-rd) * batch_x[r_index1] + rd * batch_x[r_index2]\n",
        "    batch_y_mixup = (1-rd) * batch_y[r_index1] + rd * batch_y[r_index2]\n",
        "    return batch_x_mixup, batch_y_mixup\n",
        "\n",
        "def noise(batch_x, batch_y, random_max=0.1):\n",
        "    size = batch_x.shape\n",
        "    batch_x_noise = batch_x + np.random.uniform(0,random_max,size=size)\n",
        "    batch_y_noise = batch_y\n",
        "    return batch_x_noise, batch_y_noise\n",
        "\n",
        "def jitter(x,y,snr_db):\n",
        "    \"\"\"\n",
        "    根据信噪比添加噪声\n",
        "    :param x:\n",
        "    :param snr_db:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # 随机选择信噪比\n",
        "    assert isinstance(snr_db, list)\n",
        "    snr_db_low = snr_db[0]\n",
        "    snr_db_up = snr_db[1]\n",
        "    snr_db = np.random.randint(snr_db_low, snr_db_up, (1,))[0]\n",
        "\n",
        "    snr = 10 ** (snr_db / 10)\n",
        "    Xp = np.sum(x ** 2, axis=0, keepdims=True) / x.shape[0]  # 计算信号功率\n",
        "    Np = Xp / snr  # 计算噪声功率\n",
        "    n = np.random.normal(size=x.shape, scale=np.sqrt(Np), loc=0.0)  # 计算噪声 loc均值，scale方差\n",
        "    xn = x + n\n",
        "    yn=y\n",
        "    return xn,yn\n",
        "# 迭代器 产生\n",
        "\n",
        "def data_generator(data_x, data_y,dataAugment=False, shuffle=False):\n",
        "\n",
        "  # 先shuffle打乱\n",
        "  if shuffle == True:\n",
        "      data_x, data_y = shuffle_data(data_x, data_y)\n",
        "\n",
        "  list_batch_x, list_batch_y = [], []\n",
        "  list_batch_x.append(data_x), list_batch_y.append(data_y)\n",
        "\n",
        "  # 数据增强 \n",
        "  if dataAugment == True:\n",
        "\n",
        "    # 数据增强 mixup beta\n",
        "    batch_x_mixup, batch_y_mixup = mixup(data_x, data_y, random_max=0.2)\n",
        "    list_batch_x.append(batch_x_mixup), list_batch_y.append(batch_y_mixup)\n",
        "    print(batch_x_mixup.shape,batch_y_mixup.shape)\n",
        "\n",
        "    # 数据增强 mixup uniform\n",
        "    batch_x_mixup_uniform, batch_y_mixup_uniform = mixup_uniform(data_x, data_y, random_max=0.1)\n",
        "    list_batch_x.append(batch_x_mixup_uniform), list_batch_y.append(batch_y_mixup_uniform)\n",
        "    print(batch_x_mixup_uniform.shape,batch_y_mixup_uniform.shape)\n",
        "\n",
        "    # 数据增强 随机正态噪声\n",
        "    batch_x_noise, batch_y_noise = noise(data_x, data_y, random_max=0.1)\n",
        "    list_batch_x.append(batch_x_noise), list_batch_y.append(batch_y_noise)\n",
        "    print(batch_x_noise.shape,batch_y_noise.shape)\n",
        "    # 数据增强 功率噪声\n",
        "    batch_x_jitter, batch_y_jitter=jitter(data_x,data_y,[5,15])\n",
        "    list_batch_x.append(batch_x_jitter), list_batch_y.append(batch_y_jitter)\n",
        "    print(batch_x_jitter.shape,batch_y_jitter.shape)\n",
        "  batch_x_yield, batch_y_yield = np.vstack(list_batch_x), np.vstack(list_batch_y)\n",
        "\n",
        "  return batch_x_yield, batch_y_yield"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "7jl6uq-FgonO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def block(input, filters, kernal_size):\n",
        "    cnn = tf.keras.layers.Conv1D(filters, 1, padding='SAME', activation='relu')(input)\n",
        "    cnn = tf.keras.layers.LayerNormalization()(cnn)\n",
        "    cnn = tf.keras.layers.Dropout(0.3)(cnn)\n",
        "\n",
        "    cnn = tf.keras.layers.Conv1D(filters, kernal_size, padding='SAME',activation='relu')(cnn)\n",
        "    cnn = tf.keras.layers.LayerNormalization()(cnn)\n",
        "    cnn = tf.keras.layers.Dropout(0.3)(cnn)\n",
        "\n",
        "    cnn = tf.keras.layers.Conv1D(filters, 1, padding='SAME', activation='relu')(cnn)\n",
        "    cnn = tf.keras.layers.LayerNormalization()(cnn)\n",
        "    input = tf.keras.layers.Conv1D(filters, 1)(input)\n",
        "    output = tf.keras.layers.Add()([input, cnn])\n",
        "    return output\n",
        "\n",
        "def block2(input, filters=128, kernal_size=5):\n",
        "    input = block(input, filters, kernal_size)\n",
        "    input = tf.keras.layers.MaxPooling1D(2)(input)\n",
        "    input = tf.keras.layers.SpatialDropout1D(0.5)(input)\n",
        "    input = block(input, filters//2, kernal_size)\n",
        "    output = tf.keras.layers.GlobalAveragePooling1D()(input)\n",
        "    return output\n",
        "\n",
        "def Simple_Resnet(input_shape, num_classes):\n",
        "    inputs = tf.keras.layers.Input(shape=input_shape[1:])\n",
        "    seq_3 = block2(inputs, kernal_size=3)\n",
        "    seq_5 = block2(inputs, kernal_size=5)\n",
        "    seq_7 = block2(inputs, kernal_size=7)\n",
        "    seq = tf.keras.layers.concatenate([seq_3, seq_5, seq_7])\n",
        "    seq = tf.keras.layers.Dense(512, activation='relu')(seq)\n",
        "    seq = tf.keras.layers.Dropout(0.3)(seq)\n",
        "    seq = tf.keras.layers.Dense(128, activation='relu')(seq)\n",
        "    seq = tf.keras.layers.Dropout(0.3)(seq)\n",
        "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(seq)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=[inputs], outputs=[outputs])\n",
        "\n",
        "    return model\n",
        "\n",
        "def conv_lstm():\n",
        "    #建立卷积层1\n",
        "    #model.add(GaussianNoise(input_shape = (60,10),stddev=0.04))\n",
        "    input = Input(shape=(60, 10))\n",
        "    hin = Input(shape=(20, ))\n",
        "    X = Conv1D(filters=256,\n",
        "                 kernel_size=5,\n",
        "                 padding='same',\n",
        "                 activation = 'relu')(input)\n",
        "    X = BatchNormalization()(X)\n",
        "    #建立卷积层2\n",
        "    X = Conv1D(filters=192,\n",
        "                 kernel_size=5,\n",
        "                 padding='same',\n",
        "                 activation = 'relu')(X)\n",
        "    X = BatchNormalization()(X)\n",
        "    X = MaxPooling1D()(X)\n",
        "    X = Dropout(0.3)(X)\n",
        "    #建立卷积层2\n",
        "    X = Conv1D(filters=128,\n",
        "                 kernel_size=5,\n",
        "                 padding='same',\n",
        "                 activation = 'relu')(X)\n",
        "    X = MaxPooling1D()(X)\n",
        "    X = Dropout(0.3)(X)   \n",
        "    #建立平坦层\n",
        "    #model.add(GlobalMaxPooling1D())\n",
        "    #建立隐蔽层\n",
        "    X = LSTM(64)(X)\n",
        "    X = BatchNormalization()(X)\n",
        "    merge = concatenate([X, hin])\n",
        "    merge = Dropout(0.35)(merge) \n",
        "    merge = Dense(32, activation='relu')(merge)####\n",
        "    y = Dense(19, activation='softmax')(merge)\n",
        "\n",
        "class CustomModel(tf.keras.Model):\n",
        "    '''自定义模型'''\n",
        "    def __init__(self):\n",
        "        '''初始化模型层'''\n",
        "        super().__init__()\n",
        "        self.conv1 = tf.keras.layers.Conv1D(\n",
        "            filters=64, kernel_size=3,strides=1,padding='same',activation='relu')\n",
        "        self.batchnorm1=tf.keras.layers.BatchNormalization()\n",
        "        self.disout1 = Disout(0.3, block_size=3)\n",
        "\n",
        "\n",
        "        self.conv2 = tf.keras.layers.Conv1D(\n",
        "            filters=128, kernel_size=3,strides=1,padding='same',activation='relu')\n",
        "        self.batchnorm2=tf.keras.layers.BatchNormalization()\n",
        "        self.disout2 = Disout(0.3, block_size=3)\n",
        "        self.pool1 = tf.keras.layers.MaxPool1D()\n",
        "\n",
        "        self.conv3 = tf.keras.layers.Conv1D(\n",
        "            filters=256, kernel_size=3,strides=1,padding='valid',activation='relu')\n",
        "        #,kernel_regularizer=tf.keras.regularizers.l1_l2(0.01)\n",
        "        self.batchnorm3=tf.keras.layers.BatchNormalization()\n",
        "        self.disout3 = Disout(0.3, block_size=3)\n",
        "        self.pool2 = tf.keras.layers.MaxPool1D()\n",
        "\n",
        "        self.conv4 = tf.keras.layers.Conv1D(\n",
        "            filters=256, kernel_size=3,strides=1,padding='same',activation='relu')\n",
        "        self.batchnorm4=tf.keras.layers.BatchNormalization()\n",
        "        self.disout4 = Disout(0.3, block_size=3)\n",
        "\n",
        "        self.flatten = tf.keras.layerstf.keras.layers.MaxPool1D()\n",
        "        self.disout5 = Disout1D(0.5, block_size=1)\n",
        "        self.dropout= tf.keras.layers.Dropout(0.3)\n",
        "        self.fc = tf.keras.layers.Dense(19, activation='softmax')\n",
        "\n",
        "    def call(self, x):\n",
        "        '''运算部分'''\n",
        "        x=self.conv1(x)\n",
        "        x=self.batchnorm1(x)\n",
        "        #x=self.disout1(x)\n",
        "\n",
        "\n",
        "        x=self.conv2(x)\n",
        "        x=self.batchnorm2(x)\n",
        "        #x=self.disout2(x)\n",
        "        x=self.pool1(x)\n",
        "\n",
        "        x=self.conv3(x)\n",
        "        x=self.batchnorm3(x)\n",
        "        #x=self.disout3(x)\n",
        "        #x=self.pool2(x)\n",
        "\n",
        "        x=self.conv4(x)\n",
        "        x=self.batchnorm4(x)\n",
        "        #x=self.disout4(x)\n",
        "\n",
        "        x=self.flatten(x)\n",
        "        x=self.dropout(x)\n",
        "        x=self.fc(x)\n",
        "        return x\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "BHx_opiWgonU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f626b39d-4b00-4a3a-de6b-64bd7e0c3b3d"
      },
      "source": [
        "proba_t = np.zeros((7500, 19))\n",
        "val_loss = []\n",
        "val_acc = []\n",
        "val_acc_combo=[]\n",
        "for fold, (xx, yy) in enumerate(kfold.split(x, y_cls)):\n",
        "    y_cls_ = tf.keras.utils.to_categorical(y_cls, num_classes=19)\n",
        "    y_sence_ = tf.keras.utils.to_categorical(y_scene, num_classes=4)\n",
        "    y_action_ = tf.keras.utils.to_categorical(y_action, num_classes=7)\n",
        "    #model=CustomModel()\n",
        "    model=Simple_Resnet(x.shape, 19)\n",
        "    #model = Conv_Lstm()\n",
        "    model.compile(loss=tf.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
        "                  #\"categorical_crossentropy\"\n",
        "                  optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                  metrics=['acc',get_acc_combo()])\n",
        "    plateau = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_acc_combo\",\n",
        "                                verbose=1,\n",
        "                                mode='max',\n",
        "                                factor=0.1,\n",
        "                                patience=3)\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_acc_combo',\n",
        "                                   verbose=1,\n",
        "                                   mode='max',\n",
        "                                   patience=20)\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint(f'fold{fold}.h5',\n",
        "                                 monitor='val_acc_combo',\n",
        "                                 verbose=1,\n",
        "                                 mode='max',\n",
        "                                 save_best_only=True)\n",
        "    fold_x,fold_y=data_generator(x[xx],y_cls_[xx], dataAugment=True, shuffle=True)\n",
        "    model.summary()\n",
        "    print(\"fold\",fold)\n",
        "    hist = model.fit(\n",
        "              fold_x,fold_y,\n",
        "              batch_size=64,\n",
        "              epochs=150,\n",
        "              verbose=1,\n",
        "              shuffle=True,\n",
        "              validation_data=(x[yy], y_cls_[yy]),\n",
        "              callbacks=[plateau, early_stopping, checkpoint])\n",
        "     \n",
        "    val_loss.append(np.min(hist.history['val_loss']))\n",
        "    val_acc.append(np.max(hist.history['val_acc']))\n",
        "    val_acc_combo.append(np.max(hist.history['val_acc_combo']))\n",
        "    model.load_weights(f'fold{fold}.h5')\n",
        "    proba_t+= model.predict(t, verbose=1, batch_size=1024) / 10."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10612, 60, 8) (10612, 19)\n",
            "(10612, 60, 8) (10612, 19)\n",
            "(10612, 60, 8) (10612, 19)\n",
            "(10612, 60, 8) (10612, 19)\n",
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 60, 8)]      0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_4 (Conv1D)               (None, 60, 128)      1152        input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_12 (Conv1D)              (None, 60, 128)      1152        input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_20 (Conv1D)              (None, 60, 128)      1152        input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_3 (LayerNor (None, 60, 128)      256         conv1d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_9 (LayerNor (None, 60, 128)      256         conv1d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_15 (LayerNo (None, 60, 128)      256         conv1d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 60, 128)      0           layer_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 60, 128)      0           layer_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 60, 128)      0           layer_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_5 (Conv1D)               (None, 60, 128)      49280       dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_13 (Conv1D)              (None, 60, 128)      82048       dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_21 (Conv1D)              (None, 60, 128)      114816      dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_4 (LayerNor (None, 60, 128)      256         conv1d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_10 (LayerNo (None, 60, 128)      256         conv1d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_16 (LayerNo (None, 60, 128)      256         conv1d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 60, 128)      0           layer_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 60, 128)      0           layer_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 60, 128)      0           layer_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, 60, 128)      16512       dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_14 (Conv1D)              (None, 60, 128)      16512       dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_22 (Conv1D)              (None, 60, 128)      16512       dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_7 (Conv1D)               (None, 60, 128)      1152        input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_5 (LayerNor (None, 60, 128)      256         conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_15 (Conv1D)              (None, 60, 128)      1152        input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_11 (LayerNo (None, 60, 128)      256         conv1d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_23 (Conv1D)              (None, 60, 128)      1152        input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_17 (LayerNo (None, 60, 128)      256         conv1d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 60, 128)      0           conv1d_7[0][0]                   \n",
            "                                                                 layer_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 60, 128)      0           conv1d_15[0][0]                  \n",
            "                                                                 layer_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 60, 128)      0           conv1d_23[0][0]                  \n",
            "                                                                 layer_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D)    (None, 30, 128)      0           add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1D)  (None, 30, 128)      0           add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1D)  (None, 30, 128)      0           add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d (SpatialDropo (None, 30, 128)      0           max_pooling1d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_1 (SpatialDro (None, 30, 128)      0           max_pooling1d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "spatial_dropout1d_2 (SpatialDro (None, 30, 128)      0           max_pooling1d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_8 (Conv1D)               (None, 30, 64)       8256        spatial_dropout1d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_16 (Conv1D)              (None, 30, 64)       8256        spatial_dropout1d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_24 (Conv1D)              (None, 30, 64)       8256        spatial_dropout1d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_6 (LayerNor (None, 30, 64)       128         conv1d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_12 (LayerNo (None, 30, 64)       128         conv1d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_18 (LayerNo (None, 30, 64)       128         conv1d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 30, 64)       0           layer_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 30, 64)       0           layer_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 30, 64)       0           layer_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_9 (Conv1D)               (None, 30, 64)       12352       dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_17 (Conv1D)              (None, 30, 64)       20544       dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_25 (Conv1D)              (None, 30, 64)       28736       dropout_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_7 (LayerNor (None, 30, 64)       128         conv1d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_13 (LayerNo (None, 30, 64)       128         conv1d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_19 (LayerNo (None, 30, 64)       128         conv1d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 30, 64)       0           layer_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 30, 64)       0           layer_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_13 (Dropout)            (None, 30, 64)       0           layer_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_10 (Conv1D)              (None, 30, 64)       4160        dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_18 (Conv1D)              (None, 30, 64)       4160        dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_26 (Conv1D)              (None, 30, 64)       4160        dropout_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_11 (Conv1D)              (None, 30, 64)       8256        spatial_dropout1d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_8 (LayerNor (None, 30, 64)       128         conv1d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_19 (Conv1D)              (None, 30, 64)       8256        spatial_dropout1d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_14 (LayerNo (None, 30, 64)       128         conv1d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_27 (Conv1D)              (None, 30, 64)       8256        spatial_dropout1d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_20 (LayerNo (None, 30, 64)       128         conv1d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 30, 64)       0           conv1d_11[0][0]                  \n",
            "                                                                 layer_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 30, 64)       0           conv1d_19[0][0]                  \n",
            "                                                                 layer_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 30, 64)       0           conv1d_27[0][0]                  \n",
            "                                                                 layer_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d (Globa (None, 64)           0           add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_1 (Glo (None, 64)           0           add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling1d_2 (Glo (None, 64)           0           add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 192)          0           global_average_pooling1d[0][0]   \n",
            "                                                                 global_average_pooling1d_1[0][0] \n",
            "                                                                 global_average_pooling1d_2[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 512)          98816       concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_14 (Dropout)            (None, 512)          0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 128)          65664       dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dropout_15 (Dropout)            (None, 128)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 19)           2451        dropout_15[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 596,627\n",
            "Trainable params: 596,627\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "fold 0\n",
            "Epoch 1/150\n",
            "588/830 [====================>.........] - ETA: 10s - loss: 2.0593 - acc: 0.4249 - acc_combo: 0.4993"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "md1Rw8VOBF9g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "plt.figure(figsize=(30,10))\n",
        "plt.subplot(1,3,1)\n",
        "plt.plot(val_loss,color='g')\n",
        "plt.ylim(0,2)\n",
        "plt.title('val_loss',fontsize=15)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('val_loss')\n",
        "plt.subplot(1,3,2)\n",
        "plt.plot(val_acc,color='g')\n",
        "plt.ylim(0.6,1)\n",
        "plt.title('val_acc',fontsize=15)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('val_acc')\n",
        "plt.subplot(1,3,3)\n",
        "plt.plot(val_acc_combo,color='g')\n",
        "plt.ylim(0.6,1)\n",
        "plt.title('val_acc_combo',fontsize=15)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('val_acc_combo')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        },
        "id": "1jZvFw69gonX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model=CustomModel()\n",
        "#print(model.summary())\n",
        "#model.load_weights('fold3.h5',by_name=True)\n",
        "proba_y = np.zeros((len(x), 19))\n",
        "predict=model.predict(x, verbose=1, batch_size=1024)\n",
        "proba_y=np.argmax(predict,axis=1)\n",
        "#print(train_fragment_compare.shape)\n",
        "#train_compare=pd.DataFrame(train_fragment_compare,columns=[\"fragment_id\"])\n",
        "#train_compare[\"behavior_id\"]=proba_y[0:7292]\n",
        "#train_compare.to_csv(\"data/train_predict.csv\")\n",
        "#print(proba_y)\n",
        "\n",
        "def acc_combo(y, y_pred):\n",
        "    # 数值ID与行为编码的对应关系\n",
        "    mapping = {0: 'A_0', 1: 'A_1', 2: 'A_2', 3: 'A_3', \n",
        "        4: 'D_4', 5: 'A_5', 6: 'B_1',7: 'B_5', \n",
        "        8: 'B_2', 9: 'B_3', 10: 'B_0', 11: 'A_6', \n",
        "        12: 'C_1', 13: 'C_3', 14: 'C_0', 15: 'B_6', \n",
        "        16: 'C_2', 17: 'C_5', 18: 'C_6'}\n",
        "    # 将行为ID转为编码\n",
        "    code_y, code_y_pred = mapping[y], mapping[y_pred]\n",
        "    if code_y == code_y_pred: #编码完全相同得分1.0\n",
        "        return 1.0\n",
        "    elif code_y.split(\"_\")[0] == code_y_pred.split(\"_\")[0]: #编码仅字母部分相同得分1.0/7\n",
        "        return 1.0/7\n",
        "    elif code_y.split(\"_\")[1] == code_y_pred.split(\"_\")[1]: #编码仅数字部分相同得分1.0/3\n",
        "        return 1.0/3\n",
        "    else:\n",
        "        return 0.0\n",
        "score = sum(acc_combo(y_true, y_pred) for y_true, y_pred in zip(y_cls, proba_y)) / proba_y.shape[0]\n",
        "print(round(score, 5))\n",
        "\n",
        "sub.behavior_id=np.argmax(proba_t, axis=1)\n",
        "print(sub)\n",
        "from datetime import *\n",
        "current = datetime.now()\n",
        "current=current.strftime('%m-%d-%H-%M')\n",
        "txt_acc=np.mean(val_acc)\n",
        "txt_acc_combo=np.mean(val_acc_combo)\n",
        "np.savetxt(\"complexconv_%s_%.5f.txt\"%(current,txt_acc),proba_t)\n",
        "sub.to_csv('data/%s_sub%.5f_%.5f_%.5f.csv' %(current,score,txt_acc,txt_acc_combo), index=False)\n",
        "#生成半监督训练数据\n",
        "sub[\"proba\"]=np.max(proba_t,axis=1)\n",
        "sub.to_csv(\"data/semi_test_8_nobest_%s_%.5f.csv\"%(current,txt_acc),index=False)\n",
        "sub.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqeUDujPJhSF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "sns.set()\n",
        "vc = train.iloc[:425361,:]['behavior_id'].value_counts().sort_index()/train.iloc[:425361,:]['behavior_id'].sum()\n",
        "sns.barplot(vc.index, vc.values)\n",
        "plt.title(\"train\")\n",
        "plt.ylim(0,0.016)\n",
        "plt.show()\n",
        "vc = sub['behavior_id'].value_counts().sort_index()/sub['behavior_id'].sum()\n",
        "sns.barplot(vc.index, vc.values)\n",
        "plt.title(\"test\")\n",
        "plt.ylim(0,0.016)\n",
        "plt.show()\n",
        "stacking=pd.read_csv(\"data/stacking_200801.csv\")\n",
        "vc = stacking['behavior_id'].value_counts().sort_index()/sub['behavior_id'].sum()\n",
        "sns.barplot(vc.index, vc.values)\n",
        "plt.title(\"stacking\")\n",
        "plt.ylim(0,0.016)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7gppVx1jGUo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
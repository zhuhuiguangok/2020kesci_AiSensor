{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNN_plus_feature.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"yKJhknGN67LF","colab_type":"text","pycharm":{}},"source":["Tips"]},{"cell_type":"code","metadata":{"id":"TY7bJltCFcXJ","colab_type":"code","pycharm":{},"colab":{}},"source":["import numpy as np\n","import os\n","import pandas as pd\n","from tqdm import tqdm\n","from scipy.signal import resample\n","# 基于 tensorflow.keras\n","from tensorflow.keras.layers import *\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.utils import to_categorical\n","from sklearn.model_selection import StratifiedKFold\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9VJBChwCF3oR","colab_type":"code","pycharm":{},"colab":{}},"source":["txdir='/content/drive/My Drive/Colab Notebooks'\n","os.chdir(txdir)\n","sub = pd.read_csv('data/submit.csv')\n","df_train = pd.read_csv('data/sensor_train.csv')\n","df_test  = pd.read_csv('data/sensor_test.csv')\n","y = df_train.groupby('fragment_id')['behavior_id'].min()\n","\n","df_test['fragment_id'] += 10000\n","df_data = pd.concat([df_train, df_test],axis=0,ignore_index=True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1yTau59ELkjX","colab_type":"code","pycharm":{},"colab":{"base_uri":"https://localhost:8080/","height":194},"executionInfo":{"status":"ok","timestamp":1596101793007,"user_tz":-480,"elapsed":2238,"user":{"displayName":"guang wayne","photoUrl":"","userId":"12121093145507417035"}},"outputId":"5774ef8f-cd69-4c3b-e60c-5e00afafd4af"},"source":["df = df_data.drop_duplicates(subset=['fragment_id']).reset_index(drop=True)[['fragment_id', 'behavior_id']]\n","df.head()\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>fragment_id</th>\n","      <th>behavior_id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   fragment_id  behavior_id\n","0            0          0.0\n","1            1          0.0\n","2            2          0.0\n","3            3          0.0\n","4            4          0.0"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"code","metadata":{"id":"j3evS1ZgFc5v","colab_type":"code","pycharm":{},"colab":{}},"source":["df_data['acc'] = (df_data['acc_x'] ** 2 + df_data['acc_y'] ** 2 + df_data['acc_z'] ** 2) ** 0.5\n","df_data['accg'] = (df_data['acc_xg'] ** 2 + df_data['acc_yg'] ** 2 + df_data['acc_zg'] ** 2) ** 0.5\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yQ4cgF-AGKpX","colab_type":"code","pycharm":{},"colab":{"base_uri":"https://localhost:8080/","height":124},"executionInfo":{"status":"ok","timestamp":1596101794106,"user_tz":-480,"elapsed":2311,"user":{"displayName":"guang wayne","photoUrl":"","userId":"12121093145507417035"}},"outputId":"089a231e-a2b4-4c23-fd1c-e9d453c1edbe"},"source":["label_feat = 'behavior_id'\n","train = df_data[df_data[label_feat].isna()==False]\n","test = df_data[df_data[label_feat].isna()==True]\n","test['fragment_id'] -=10000\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  after removing the cwd from sys.path.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"Ek-jexAKGYLZ","colab_type":"code","pycharm":{},"colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1596101831832,"user_tz":-480,"elapsed":39631,"user":{"displayName":"guang wayne","photoUrl":"","userId":"12121093145507417035"}},"outputId":"363504ee-a792-44d0-e4c8-1c5c2cf69b82"},"source":["x = np.zeros((7292, 60, 8, 1))\n","t = np.zeros((7500, 60, 8, 1))\n","for i in tqdm(range(7292)):\n","    tmp = train[train.fragment_id == i][:60]\n","    x[i,:,:, 0] = resample(tmp.drop(['fragment_id', 'time_point', 'behavior_id'],\n","                                    axis=1), 60, np.array(tmp.time_point))[0]\n","for i in tqdm(range(7500)):\n","    tmp = test[test.fragment_id == i][:60]\n","    t[i,:,:, 0] = resample(tmp.drop(['fragment_id', 'time_point', 'behavior_id'],\n","                                    axis=1), 60, np.array(tmp.time_point))[0]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 7292/7292 [00:18<00:00, 391.99it/s]\n","100%|██████████| 7500/7500 [00:19<00:00, 392.92it/s]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"bDrDdLh3LTXR","colab_type":"text","pycharm":{}},"source":["# 提取简单统计特征"]},{"cell_type":"code","metadata":{"id":"c13EMpb_Qo1u","colab_type":"code","pycharm":{},"colab":{}},"source":["df_data['xy'] = (df_data['acc_x'] ** 2 + df_data['acc_y'] ** 2) ** 0.5\n","df_data['xy_g'] = (df_data['acc_xg'] ** 2 + df_data['acc_yg'] ** 2) ** 0.5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dNaHj2JeGyGh","colab_type":"code","pycharm":{},"colab":{}},"source":["def get_dic(df,  main_col, fea_col, agg):\n","    dic = df.groupby(main_col)[fea_col].agg(agg).to_dict()\n","    fea_name = '_'.join([main_col, fea_col, agg])\n","    return fea_name, dic\n","    \n","def get_1st_order_xyz_features(df, fea_cols, main_col = 'fragment_id'): \n","    df_fea           = pd.DataFrame()\n","    df_fea[main_col] = df[main_col].unique()\n","    ## count 特征 ##\n","    _, dic = get_dic(df, main_col, fea_cols[0], 'count') \n","    df_fea['cnt']    = df_fea[main_col].map(dic).values\n","    \n","    ## 数值统计特征 ##\n","    for f in tqdm(fea_cols):\n","        for agg in ['min','max','mean','std','skew','median']:\n","\n","            fea_name, dic       = get_dic(df, main_col, f, agg) \n","            df_fea[fea_name]    = df_fea[main_col].map(dic).values\n","            \n","        df_fea['_'.join([main_col, f, 'gap'])]   = df_fea['_'.join([main_col, f, 'max'])] - df_fea['_'.join([main_col, f, 'min'])]\n","        df_fea['_'.join([main_col, f, 'skew2'])] = (df_fea['_'.join([main_col, f, 'mean'])] - df_fea['_'.join([main_col, f, 'median'])]) / df_fea['_'.join([main_col, f, 'std'])]\n","        \n","    return df_fea\n","\n","def get_1st_order_xyz_features_self(df, fea_cols, main_col = 'fragment_id'):\n","    \n","    df_fea  = pd.DataFrame()\n","    ## 数值统计特征 ##\n","    for f in tqdm(fea_cols):\n","        df_fea=df.groupby(main_col).agg(\n","            min=(f,'min'),\n","            max=(f,'max'),\n","            mean=(f,'mean'),\n","            std=(f,'std'),\n","            skew=(f,'skew'),\n","            median=(f,'median'),\n","        ).reset_index().rename(columns={\n","            \"min\":'_'.join([main_col, f, 'min']),\n","            \"max\":'_'.join([main_col, f, 'max']),\n","            \"mean\":'_'.join([main_col, f, 'mean']),\n","            \"std\":'_'.join([main_col, f, 'std']),\n","            \"skew\":'_'.join([main_col, f, 'skew']),\n","            \"median\":'_'.join([main_col, f, 'median']),\n","        })\n","        df_fea['_'.join([main_col, f, 'gap'])]   = df_fea['_'.join([main_col, f, 'max'])] - df_fea['_'.join([main_col, f, 'min'])]\n","        df_fea['_'.join([main_col, f, 'skew2'])] = (df_fea['_'.join([main_col, f, 'mean'])] - df_fea['_'.join([main_col, f, 'median'])]) / df_fea['_'.join([main_col, f, 'std'])]\n","         \n","    ## count 特征 ##   \n","    df_fea[main_col] = df[main_col].unique()\n","    df_fea['cnt']=df.groupby(main_col)[fea_cols[0]].agg(\"count\").values\n","    \n","    return df_fea\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dfpikd5jGyvD","colab_type":"code","pycharm":{},"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1596101882477,"user_tz":-480,"elapsed":89155,"user":{"displayName":"guang wayne","photoUrl":"","userId":"12121093145507417035"}},"outputId":"0880852f-e841-46bc-d612-761d5f333bb0"},"source":["origin_fea_cols = ['acc_x','acc_y','acc_z','acc','acc_xg','acc_yg','acc_zg','accg','xy','xy_g']\n","df_xyz_fea1 = get_1st_order_xyz_features(df_data,origin_fea_cols,main_col='fragment_id')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 10/10 [00:50<00:00,  5.00s/it]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"bCPMs9XPMq0y","colab_type":"code","pycharm":{},"colab":{"base_uri":"https://localhost:8080/","height":399},"executionInfo":{"status":"ok","timestamp":1596101882479,"user_tz":-480,"elapsed":88912,"user":{"displayName":"guang wayne","photoUrl":"","userId":"12121093145507417035"}},"outputId":"1c80458f-8772-4448-99e0-6e4098eb95e5"},"source":["df_data"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>fragment_id</th>\n","      <th>time_point</th>\n","      <th>acc_x</th>\n","      <th>acc_y</th>\n","      <th>acc_z</th>\n","      <th>acc_xg</th>\n","      <th>acc_yg</th>\n","      <th>acc_zg</th>\n","      <th>behavior_id</th>\n","      <th>acc</th>\n","      <th>accg</th>\n","      <th>xy</th>\n","      <th>xy_g</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>27</td>\n","      <td>0.3</td>\n","      <td>-0.3</td>\n","      <td>0.1</td>\n","      <td>0.6</td>\n","      <td>4.5</td>\n","      <td>8.8</td>\n","      <td>0.0</td>\n","      <td>0.435890</td>\n","      <td>9.902020</td>\n","      <td>0.424264</td>\n","      <td>4.539824</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>108</td>\n","      <td>0.1</td>\n","      <td>-0.0</td>\n","      <td>-0.4</td>\n","      <td>0.4</td>\n","      <td>4.7</td>\n","      <td>8.4</td>\n","      <td>0.0</td>\n","      <td>0.412311</td>\n","      <td>9.633795</td>\n","      <td>0.100000</td>\n","      <td>4.716991</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>198</td>\n","      <td>0.1</td>\n","      <td>0.0</td>\n","      <td>0.3</td>\n","      <td>0.9</td>\n","      <td>4.6</td>\n","      <td>9.0</td>\n","      <td>0.0</td>\n","      <td>0.316228</td>\n","      <td>10.147413</td>\n","      <td>0.100000</td>\n","      <td>4.687217</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>297</td>\n","      <td>0.1</td>\n","      <td>-0.1</td>\n","      <td>-0.5</td>\n","      <td>0.8</td>\n","      <td>4.7</td>\n","      <td>7.2</td>\n","      <td>0.0</td>\n","      <td>0.519615</td>\n","      <td>8.635392</td>\n","      <td>0.141421</td>\n","      <td>4.767599</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>388</td>\n","      <td>0.1</td>\n","      <td>0.2</td>\n","      <td>0.6</td>\n","      <td>0.9</td>\n","      <td>4.7</td>\n","      <td>8.9</td>\n","      <td>0.0</td>\n","      <td>0.640312</td>\n","      <td>10.104949</td>\n","      <td>0.223607</td>\n","      <td>4.785394</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>855536</th>\n","      <td>17499</td>\n","      <td>4611</td>\n","      <td>-0.2</td>\n","      <td>-0.1</td>\n","      <td>-0.2</td>\n","      <td>-2.0</td>\n","      <td>2.4</td>\n","      <td>9.0</td>\n","      <td>NaN</td>\n","      <td>0.300000</td>\n","      <td>9.526804</td>\n","      <td>0.223607</td>\n","      <td>3.124100</td>\n","    </tr>\n","    <tr>\n","      <th>855537</th>\n","      <td>17499</td>\n","      <td>4692</td>\n","      <td>0.1</td>\n","      <td>0.1</td>\n","      <td>-1.1</td>\n","      <td>-1.6</td>\n","      <td>2.8</td>\n","      <td>8.2</td>\n","      <td>NaN</td>\n","      <td>1.109054</td>\n","      <td>8.811356</td>\n","      <td>0.141421</td>\n","      <td>3.224903</td>\n","    </tr>\n","    <tr>\n","      <th>855538</th>\n","      <td>17499</td>\n","      <td>4788</td>\n","      <td>-0.5</td>\n","      <td>-0.1</td>\n","      <td>1.0</td>\n","      <td>-1.5</td>\n","      <td>2.1</td>\n","      <td>10.5</td>\n","      <td>NaN</td>\n","      <td>1.122497</td>\n","      <td>10.812493</td>\n","      <td>0.509902</td>\n","      <td>2.580698</td>\n","    </tr>\n","    <tr>\n","      <th>855539</th>\n","      <td>17499</td>\n","      <td>4873</td>\n","      <td>-0.3</td>\n","      <td>0.2</td>\n","      <td>-0.3</td>\n","      <td>-1.6</td>\n","      <td>2.2</td>\n","      <td>9.3</td>\n","      <td>NaN</td>\n","      <td>0.469042</td>\n","      <td>9.689685</td>\n","      <td>0.360555</td>\n","      <td>2.720294</td>\n","    </tr>\n","    <tr>\n","      <th>855540</th>\n","      <td>17499</td>\n","      <td>4967</td>\n","      <td>-0.2</td>\n","      <td>0.0</td>\n","      <td>0.1</td>\n","      <td>-1.6</td>\n","      <td>2.1</td>\n","      <td>9.5</td>\n","      <td>NaN</td>\n","      <td>0.223607</td>\n","      <td>9.860020</td>\n","      <td>0.200000</td>\n","      <td>2.640076</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>855541 rows × 13 columns</p>\n","</div>"],"text/plain":["        fragment_id  time_point  acc_x  ...       accg        xy      xy_g\n","0                 0          27    0.3  ...   9.902020  0.424264  4.539824\n","1                 0         108    0.1  ...   9.633795  0.100000  4.716991\n","2                 0         198    0.1  ...  10.147413  0.100000  4.687217\n","3                 0         297    0.1  ...   8.635392  0.141421  4.767599\n","4                 0         388    0.1  ...  10.104949  0.223607  4.785394\n","...             ...         ...    ...  ...        ...       ...       ...\n","855536        17499        4611   -0.2  ...   9.526804  0.223607  3.124100\n","855537        17499        4692    0.1  ...   8.811356  0.141421  3.224903\n","855538        17499        4788   -0.5  ...  10.812493  0.509902  2.580698\n","855539        17499        4873   -0.3  ...   9.689685  0.360555  2.720294\n","855540        17499        4967   -0.2  ...   9.860020  0.200000  2.640076\n","\n","[855541 rows x 13 columns]"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"markdown","metadata":{"id":"FAUl-McqMNLY","colab_type":"text","pycharm":{}},"source":["# 提取傅里叶特征"]},{"cell_type":"markdown","metadata":{"id":"Nxfzy087NdRd","colab_type":"text","pycharm":{}},"source":[""]},{"cell_type":"code","metadata":{"id":"Lro2ccDjG26z","colab_type":"code","pycharm":{},"colab":{}},"source":["from scipy.fftpack import fft\n","from scipy.signal import resample\n","import matplotlib.pyplot as plt\n","def get_fft_values(y_values, N, f_s):\n","    f_values = np.linspace(0.0, f_s/2.0, N//2)\n","    fft_values_ = fft(y_values)\n","    fft_values = 2.0/N * np.abs(fft_values_[0:N//2])\n","    return f_values, fft_values\n","\n","\n","from scipy.signal import welch\n","def get_psd_values(y_values, N, f_s):\n","    f_values, psd_values = welch(y_values, fs=f_s)\n","    return f_values, psd_values\n","\n","\n","def show_topK_peak(fft_values,top_peak_number=5):  \n","    '''\n","    show_topK_peak(fft_values)\n","    '''\n","    peak_list = []\n","    for index,values in enumerate(fft_values):\n","        if index == 0 or index == len(fft_values) - 1:\n","            continue\n","        if fft_values[index] > fft_values[index - 1] and fft_values[index] > fft_values[index + 1]:\n","            peak_list.append((values,index))\n","    t_res = sorted(peak_list)[::-1]\n","#     print(t_res)\n","#     t_res = sorted(zip(fft_values,range(len(fft_values))))[::-1]\n","#     print(t_res)\n","    top_peak_A = [A for A,P in t_res[:top_peak_number]]\n","    top_peak_P = [P for A,P in t_res[:top_peak_number]]\n","#     print(top_peak_A)\n","    plt.plot(range(len(fft_values)),fft_values)\n","    plt.scatter(top_peak_P,top_peak_A)\n","    plt.show()\n","    \n","    \n","top_peak_number = 5    \n","def get_fft_topK_AP(array_with_time,feat_name,K=top_peak_number):\n","#     print(array_with_time)\n","    x,t = resample(array_with_time[feat_name], 120, np.array(array_with_time[\"time_point\"]))\n","    f_values, fft_values = get_fft_values(x, N=120, f_s=5)\n","    peak_list = []\n","    for index,values in enumerate(fft_values):\n","        if index == 0 or index == len(fft_values) - 1:\n","            continue\n","        if fft_values[index] > fft_values[index - 1] and fft_values[index] > fft_values[index + 1]:\n","            peak_list.append((values,index))\n","    if len(peak_list) < 5:\n","        cnt = 5 - len(peak_list)\n","        for i in range(cnt):\n","            peak_list.append((0,-1))\n","    t_res = sorted(zip(fft_values,range(len(fft_values))))[::-1]\n","#     print(t_res)\n","    top_peak_A = [A for A,P in t_res[:top_peak_number]]\n","    top_peak_P = [P for A,P in t_res[:top_peak_number]]\n","    return [top_peak_A + top_peak_P]\n","\n","\n","top_peak_number = 5    \n","def get_psd_topK_AP(array_with_time,feat_name,K=top_peak_number):\n","#     print(array_with_time)\n","    x,t = resample(array_with_time[feat_name], 120, np.array(array_with_time[\"time_point\"]))\n","    f_values, fft_values = get_psd_values(x, N=120, f_s=5)\n","    peak_list = []\n","    for index,values in enumerate(fft_values):\n","        if index == 0 or index == len(fft_values) - 1:\n","            continue\n","        if fft_values[index] > fft_values[index - 1] and fft_values[index] > fft_values[index + 1]:\n","            peak_list.append((values,index))\n","    if len(peak_list) < 5:\n","        cnt = 5 - len(peak_list)\n","        for i in range(cnt):\n","            peak_list.append((0,-1))\n","    \n","    t_res = sorted(zip(fft_values,range(len(fft_values))))[::-1]\n","\n","    top_peak_A = [A for A,P in t_res[:top_peak_number]]\n","    top_peak_P = [P for A,P in t_res[:top_peak_number]]\n","     \n","    '''\n","    (19) Root mean square of the differences between two successive peaks;\n","    (20) Standard deviation of the intervals between two successive peaks;\n","    (21) The number of pairs of successive peaks intervals that differ by more than 50 ms.\n","    '''\n","    t_res2 = sorted(peak_list,key=lambda x:x[1])[::-1]\n","    diff_of_successive_peaks = np.zeros(len(t_res2) - 1)\n","    intervals_of_successive_peaks = np.zeros(len(t_res2) - 1)\n","    peak_values = np.array([p for p,i in peak_list])\n","    for index,i in enumerate(t_res2):\n","        if index == 0:\n","            continue\n","        diff_of_successive_peaks[index-1] = t_res2[index][0] - t_res2[index-1][0]\n","        intervals_of_successive_peaks[index-1] = t_res2[index][1] - t_res2[index-1][1]\n","    return [top_peak_A + top_peak_P]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MmTeyGEZG_k6","colab_type":"code","pycharm":{},"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1596102197942,"user_tz":-480,"elapsed":403390,"user":{"displayName":"guang wayne","photoUrl":"","userId":"12121093145507417035"}},"outputId":"83db5387-16f0-4239-8e2b-10bcd890a7f1"},"source":["oral_item = ['acc_x','acc_y','acc_z','acc','acc_xg','acc_yg','acc_zg','accg','xy','xy_g']\n","\n","for item in tqdm(oral_item):\n","    tmp = df_data[[\"fragment_id\",item,\"time_point\"]].groupby([\"fragment_id\"],as_index=False)[item].agg(get_fft_topK_AP,feat_name=item)\n","    \n","    for A in range(top_peak_number):\n","        print(A)\n","        tmp[item+\"_fftA_\"+str(A)] = tmp[item].apply(lambda x:x[A])\n","        df = df.merge(tmp[[\"fragment_id\",item+\"_fftA_\"+str(A)]],on='fragment_id',how='left')\n","    for P in range(top_peak_number):\n","        print(P)\n","        tmp[item+\"_fftP_\"+str(P)] = tmp[item].apply(lambda x:x[top_peak_number+P])\n","        df = df.merge(tmp[[\"fragment_id\",item+\"_fftP_\"+str(P)]],on='fragment_id',how='left')\n","    \n","    tmp = df_data[[\"fragment_id\",item,\"time_point\"]].groupby([\"fragment_id\"],as_index=False)[item].agg(get_psd_topK_AP,feat_name=item)\n","    for A in range(top_peak_number):\n","        tmp[item+\"_psdA_\"+str(A)] = tmp[item].apply(lambda x:x[A])\n","        df = df.merge(tmp[[\"fragment_id\",item+\"_psdA_\"+str(A)]],on='fragment_id',how='left')\n","    for P in range(top_peak_number):\n","        tmp[item+\"_psdP_\"+str(P)] = tmp[item].apply(lambda x:x[top_peak_number+P])\n","        df = df.merge(tmp[[\"fragment_id\",item+\"_psdP_\"+str(P)]],on='fragment_id',how='left')\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/10 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["0\n","1\n","2\n","3\n","4\n","0\n","1\n","2\n","3\n","4\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/scipy/signal/spectral.py:1966: UserWarning: nperseg = 256 is greater than input length  = 120, using nperseg = 120\n","  .format(nperseg, input_length))\n"," 10%|█         | 1/10 [00:31<04:39, 31.07s/it]"],"name":"stderr"},{"output_type":"stream","text":["0\n","1\n","2\n","3\n","4\n","0\n","1\n","2\n","3\n","4\n"],"name":"stdout"},{"output_type":"stream","text":["\r 20%|██        | 2/10 [01:02<04:08, 31.04s/it]"],"name":"stderr"},{"output_type":"stream","text":["0\n","1\n","2\n","3\n","4\n","0\n","1\n","2\n","3\n","4\n"],"name":"stdout"},{"output_type":"stream","text":["\r 30%|███       | 3/10 [01:33<03:37, 31.07s/it]"],"name":"stderr"},{"output_type":"stream","text":["0\n","1\n","2\n","3\n","4\n","0\n","1\n","2\n","3\n","4\n"],"name":"stdout"},{"output_type":"stream","text":["\r 40%|████      | 4/10 [02:04<03:06, 31.13s/it]"],"name":"stderr"},{"output_type":"stream","text":["0\n","1\n","2\n","3\n","4\n","0\n","1\n","2\n","3\n","4\n"],"name":"stdout"},{"output_type":"stream","text":["\r 50%|█████     | 5/10 [02:36<02:36, 31.27s/it]"],"name":"stderr"},{"output_type":"stream","text":["0\n","1\n","2\n","3\n","4\n","0\n","1\n","2\n","3\n","4\n"],"name":"stdout"},{"output_type":"stream","text":["\r 60%|██████    | 6/10 [03:07<02:05, 31.36s/it]"],"name":"stderr"},{"output_type":"stream","text":["0\n","1\n","2\n","3\n","4\n","0\n","1\n","2\n","3\n","4\n"],"name":"stdout"},{"output_type":"stream","text":["\r 70%|███████   | 7/10 [03:39<01:34, 31.54s/it]"],"name":"stderr"},{"output_type":"stream","text":["0\n","1\n","2\n","3\n","4\n","0\n","1\n","2\n","3\n","4\n"],"name":"stdout"},{"output_type":"stream","text":["\r 80%|████████  | 8/10 [04:11<01:03, 31.62s/it]"],"name":"stderr"},{"output_type":"stream","text":["0\n","1\n","2\n","3\n","4\n","0\n","1\n","2\n","3\n","4\n"],"name":"stdout"},{"output_type":"stream","text":["\r 90%|█████████ | 9/10 [04:43<00:31, 31.78s/it]"],"name":"stderr"},{"output_type":"stream","text":["0\n","1\n","2\n","3\n","4\n","0\n","1\n","2\n","3\n","4\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 10/10 [05:15<00:00, 31.55s/it]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"CAU5BE_Y7-KN","colab_type":"text","pycharm":{}},"source":["# 将简单特征与傅里叶特征合并"]},{"cell_type":"code","metadata":{"id":"OeoJWT4FHYq9","colab_type":"code","pycharm":{},"colab":{}},"source":["df_tr_te = df.merge(df_xyz_fea1, on ='fragment_id', how = 'left')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z7GX8FHn6-aQ","colab_type":"text","pycharm":{}},"source":["# 保存统计特征"]},{"cell_type":"code","metadata":{"id":"ZpnqjqSN68ah","colab_type":"code","pycharm":{},"colab":{}},"source":["data_path = '/content/drive/My Drive/Colab Notebooks'\n","df_tr_te.to_pickle(data_path + \"df_fea1.pkl\")#保存一阶统计特征以及异常点个数"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QoiLKKQJ9Kxl","colab_type":"code","pycharm":{},"colab":{}},"source":["# 后续可以直接读取统计特征"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"16EXbA5J7YHp","colab_type":"code","pycharm":{},"colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"status":"ok","timestamp":1596102198389,"user_tz":-480,"elapsed":402062,"user":{"displayName":"guang wayne","photoUrl":"","userId":"12121093145507417035"}},"outputId":"ddf5be1c-b71a-447f-8cad-8a2d3b07477a"},"source":["df_tr_te = pd.read_pickle(data_path+\"df_fea1.pkl\")\n","df_tr_te"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>fragment_id</th>\n","      <th>behavior_id</th>\n","      <th>acc_x_fftA_0</th>\n","      <th>acc_x_fftA_1</th>\n","      <th>acc_x_fftA_2</th>\n","      <th>acc_x_fftA_3</th>\n","      <th>acc_x_fftA_4</th>\n","      <th>acc_x_fftP_0</th>\n","      <th>acc_x_fftP_1</th>\n","      <th>acc_x_fftP_2</th>\n","      <th>acc_x_fftP_3</th>\n","      <th>acc_x_fftP_4</th>\n","      <th>acc_x_psdA_0</th>\n","      <th>acc_x_psdA_1</th>\n","      <th>acc_x_psdA_2</th>\n","      <th>acc_x_psdA_3</th>\n","      <th>acc_x_psdA_4</th>\n","      <th>acc_x_psdP_0</th>\n","      <th>acc_x_psdP_1</th>\n","      <th>acc_x_psdP_2</th>\n","      <th>acc_x_psdP_3</th>\n","      <th>acc_x_psdP_4</th>\n","      <th>acc_y_fftA_0</th>\n","      <th>acc_y_fftA_1</th>\n","      <th>acc_y_fftA_2</th>\n","      <th>acc_y_fftA_3</th>\n","      <th>acc_y_fftA_4</th>\n","      <th>acc_y_fftP_0</th>\n","      <th>acc_y_fftP_1</th>\n","      <th>acc_y_fftP_2</th>\n","      <th>acc_y_fftP_3</th>\n","      <th>acc_y_fftP_4</th>\n","      <th>acc_y_psdA_0</th>\n","      <th>acc_y_psdA_1</th>\n","      <th>acc_y_psdA_2</th>\n","      <th>acc_y_psdA_3</th>\n","      <th>acc_y_psdA_4</th>\n","      <th>acc_y_psdP_0</th>\n","      <th>acc_y_psdP_1</th>\n","      <th>acc_y_psdP_2</th>\n","      <th>...</th>\n","      <th>fragment_id_acc_yg_min</th>\n","      <th>fragment_id_acc_yg_max</th>\n","      <th>fragment_id_acc_yg_mean</th>\n","      <th>fragment_id_acc_yg_std</th>\n","      <th>fragment_id_acc_yg_skew</th>\n","      <th>fragment_id_acc_yg_median</th>\n","      <th>fragment_id_acc_yg_gap</th>\n","      <th>fragment_id_acc_yg_skew2</th>\n","      <th>fragment_id_acc_zg_min</th>\n","      <th>fragment_id_acc_zg_max</th>\n","      <th>fragment_id_acc_zg_mean</th>\n","      <th>fragment_id_acc_zg_std</th>\n","      <th>fragment_id_acc_zg_skew</th>\n","      <th>fragment_id_acc_zg_median</th>\n","      <th>fragment_id_acc_zg_gap</th>\n","      <th>fragment_id_acc_zg_skew2</th>\n","      <th>fragment_id_accg_min</th>\n","      <th>fragment_id_accg_max</th>\n","      <th>fragment_id_accg_mean</th>\n","      <th>fragment_id_accg_std</th>\n","      <th>fragment_id_accg_skew</th>\n","      <th>fragment_id_accg_median</th>\n","      <th>fragment_id_accg_gap</th>\n","      <th>fragment_id_accg_skew2</th>\n","      <th>fragment_id_xy_min</th>\n","      <th>fragment_id_xy_max</th>\n","      <th>fragment_id_xy_mean</th>\n","      <th>fragment_id_xy_std</th>\n","      <th>fragment_id_xy_skew</th>\n","      <th>fragment_id_xy_median</th>\n","      <th>fragment_id_xy_gap</th>\n","      <th>fragment_id_xy_skew2</th>\n","      <th>fragment_id_xy_g_min</th>\n","      <th>fragment_id_xy_g_max</th>\n","      <th>fragment_id_xy_g_mean</th>\n","      <th>fragment_id_xy_g_std</th>\n","      <th>fragment_id_xy_g_skew</th>\n","      <th>fragment_id_xy_g_median</th>\n","      <th>fragment_id_xy_g_gap</th>\n","      <th>fragment_id_xy_g_skew2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.095500</td>\n","      <td>0.092020</td>\n","      <td>0.088037</td>\n","      <td>0.071845</td>\n","      <td>0.068201</td>\n","      <td>24</td>\n","      <td>2</td>\n","      <td>13</td>\n","      <td>26</td>\n","      <td>8</td>\n","      <td>0.079752</td>\n","      <td>0.075956</td>\n","      <td>0.053650</td>\n","      <td>0.048345</td>\n","      <td>0.046628</td>\n","      <td>24</td>\n","      <td>2</td>\n","      <td>23</td>\n","      <td>27</td>\n","      <td>13</td>\n","      <td>0.041967</td>\n","      <td>0.035681</td>\n","      <td>0.033780</td>\n","      <td>0.032334</td>\n","      <td>0.030917</td>\n","      <td>21</td>\n","      <td>4</td>\n","      <td>27</td>\n","      <td>23</td>\n","      <td>25</td>\n","      <td>0.014054</td>\n","      <td>0.013951</td>\n","      <td>0.009968</td>\n","      <td>0.009779</td>\n","      <td>0.008149</td>\n","      <td>24</td>\n","      <td>21</td>\n","      <td>4</td>\n","      <td>...</td>\n","      <td>4.4</td>\n","      <td>5.2</td>\n","      <td>4.773684</td>\n","      <td>0.158707</td>\n","      <td>0.284234</td>\n","      <td>4.80</td>\n","      <td>0.8</td>\n","      <td>-0.165813</td>\n","      <td>7.2</td>\n","      <td>9.3</td>\n","      <td>8.508772</td>\n","      <td>0.338738</td>\n","      <td>-0.746650</td>\n","      <td>8.5</td>\n","      <td>2.1</td>\n","      <td>0.025896</td>\n","      <td>8.635392</td>\n","      <td>10.560776</td>\n","      <td>9.784581</td>\n","      <td>0.293469</td>\n","      <td>-0.677138</td>\n","      <td>9.786726</td>\n","      <td>1.925384</td>\n","      <td>-0.007309</td>\n","      <td>0.0</td>\n","      <td>0.905539</td>\n","      <td>0.144087</td>\n","      <td>0.154493</td>\n","      <td>2.543933</td>\n","      <td>0.100000</td>\n","      <td>0.905539</td>\n","      <td>0.285368</td>\n","      <td>4.455334</td>\n","      <td>5.215362</td>\n","      <td>4.825658</td>\n","      <td>0.155398</td>\n","      <td>0.372397</td>\n","      <td>4.825971</td>\n","      <td>0.760028</td>\n","      <td>-0.002016</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0.0</td>\n","      <td>0.050000</td>\n","      <td>0.047902</td>\n","      <td>0.044440</td>\n","      <td>0.042883</td>\n","      <td>0.040711</td>\n","      <td>0</td>\n","      <td>24</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>0.026726</td>\n","      <td>0.016944</td>\n","      <td>0.016112</td>\n","      <td>0.015899</td>\n","      <td>0.015326</td>\n","      <td>21</td>\n","      <td>22</td>\n","      <td>15</td>\n","      <td>24</td>\n","      <td>1</td>\n","      <td>0.053571</td>\n","      <td>0.033169</td>\n","      <td>0.028012</td>\n","      <td>0.026694</td>\n","      <td>0.023137</td>\n","      <td>0</td>\n","      <td>22</td>\n","      <td>20</td>\n","      <td>17</td>\n","      <td>21</td>\n","      <td>0.015934</td>\n","      <td>0.015810</td>\n","      <td>0.015056</td>\n","      <td>0.014714</td>\n","      <td>0.007909</td>\n","      <td>20</td>\n","      <td>21</td>\n","      <td>19</td>\n","      <td>...</td>\n","      <td>4.6</td>\n","      <td>5.1</td>\n","      <td>4.830357</td>\n","      <td>0.123465</td>\n","      <td>0.411559</td>\n","      <td>4.80</td>\n","      <td>0.5</td>\n","      <td>0.245877</td>\n","      <td>8.1</td>\n","      <td>9.4</td>\n","      <td>8.519643</td>\n","      <td>0.241525</td>\n","      <td>0.917398</td>\n","      <td>8.5</td>\n","      <td>1.3</td>\n","      <td>0.081328</td>\n","      <td>9.441398</td>\n","      <td>10.521407</td>\n","      <td>9.819616</td>\n","      <td>0.202188</td>\n","      <td>1.053017</td>\n","      <td>9.791067</td>\n","      <td>1.080009</td>\n","      <td>0.141198</td>\n","      <td>0.0</td>\n","      <td>0.509902</td>\n","      <td>0.093610</td>\n","      <td>0.089370</td>\n","      <td>1.834842</td>\n","      <td>0.100000</td>\n","      <td>0.509902</td>\n","      <td>-0.071503</td>\n","      <td>4.617359</td>\n","      <td>5.162364</td>\n","      <td>4.879333</td>\n","      <td>0.125653</td>\n","      <td>0.417325</td>\n","      <td>4.850773</td>\n","      <td>0.545005</td>\n","      <td>0.227288</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>0.0</td>\n","      <td>0.111380</td>\n","      <td>0.101354</td>\n","      <td>0.095443</td>\n","      <td>0.081610</td>\n","      <td>0.080519</td>\n","      <td>13</td>\n","      <td>12</td>\n","      <td>28</td>\n","      <td>21</td>\n","      <td>11</td>\n","      <td>0.223735</td>\n","      <td>0.204908</td>\n","      <td>0.107606</td>\n","      <td>0.065627</td>\n","      <td>0.064981</td>\n","      <td>13</td>\n","      <td>12</td>\n","      <td>11</td>\n","      <td>16</td>\n","      <td>14</td>\n","      <td>0.077716</td>\n","      <td>0.070431</td>\n","      <td>0.060925</td>\n","      <td>0.048163</td>\n","      <td>0.046791</td>\n","      <td>16</td>\n","      <td>5</td>\n","      <td>4</td>\n","      <td>15</td>\n","      <td>18</td>\n","      <td>0.042555</td>\n","      <td>0.041412</td>\n","      <td>0.039207</td>\n","      <td>0.026196</td>\n","      <td>0.018034</td>\n","      <td>15</td>\n","      <td>16</td>\n","      <td>5</td>\n","      <td>...</td>\n","      <td>4.4</td>\n","      <td>5.4</td>\n","      <td>4.952632</td>\n","      <td>0.234601</td>\n","      <td>-0.081638</td>\n","      <td>4.90</td>\n","      <td>1.0</td>\n","      <td>0.224345</td>\n","      <td>7.2</td>\n","      <td>9.8</td>\n","      <td>8.394737</td>\n","      <td>0.433186</td>\n","      <td>-0.103604</td>\n","      <td>8.4</td>\n","      <td>2.6</td>\n","      <td>-0.012150</td>\n","      <td>8.823831</td>\n","      <td>10.813418</td>\n","      <td>9.790638</td>\n","      <td>0.355424</td>\n","      <td>-0.205059</td>\n","      <td>9.827004</td>\n","      <td>1.989586</td>\n","      <td>-0.102316</td>\n","      <td>0.0</td>\n","      <td>0.921954</td>\n","      <td>0.205445</td>\n","      <td>0.184398</td>\n","      <td>1.539513</td>\n","      <td>0.141421</td>\n","      <td>0.921954</td>\n","      <td>0.347205</td>\n","      <td>4.570558</td>\n","      <td>5.597321</td>\n","      <td>5.026851</td>\n","      <td>0.237700</td>\n","      <td>0.436153</td>\n","      <td>4.936598</td>\n","      <td>1.026763</td>\n","      <td>0.379691</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>0.0</td>\n","      <td>0.088078</td>\n","      <td>0.083340</td>\n","      <td>0.082012</td>\n","      <td>0.077895</td>\n","      <td>0.065502</td>\n","      <td>27</td>\n","      <td>6</td>\n","      <td>11</td>\n","      <td>21</td>\n","      <td>8</td>\n","      <td>0.113268</td>\n","      <td>0.071477</td>\n","      <td>0.066208</td>\n","      <td>0.060511</td>\n","      <td>0.057133</td>\n","      <td>11</td>\n","      <td>12</td>\n","      <td>21</td>\n","      <td>27</td>\n","      <td>22</td>\n","      <td>0.068327</td>\n","      <td>0.051335</td>\n","      <td>0.051253</td>\n","      <td>0.050926</td>\n","      <td>0.047608</td>\n","      <td>21</td>\n","      <td>27</td>\n","      <td>6</td>\n","      <td>8</td>\n","      <td>22</td>\n","      <td>0.045489</td>\n","      <td>0.038310</td>\n","      <td>0.028655</td>\n","      <td>0.019457</td>\n","      <td>0.014073</td>\n","      <td>21</td>\n","      <td>22</td>\n","      <td>8</td>\n","      <td>...</td>\n","      <td>4.5</td>\n","      <td>5.5</td>\n","      <td>5.049091</td>\n","      <td>0.196141</td>\n","      <td>-0.054651</td>\n","      <td>5.00</td>\n","      <td>1.0</td>\n","      <td>0.250283</td>\n","      <td>7.5</td>\n","      <td>9.5</td>\n","      <td>8.340000</td>\n","      <td>0.414371</td>\n","      <td>0.472631</td>\n","      <td>8.3</td>\n","      <td>2.0</td>\n","      <td>0.096532</td>\n","      <td>8.828363</td>\n","      <td>10.585367</td>\n","      <td>9.767121</td>\n","      <td>0.360655</td>\n","      <td>0.193839</td>\n","      <td>9.780082</td>\n","      <td>1.757004</td>\n","      <td>-0.035936</td>\n","      <td>0.0</td>\n","      <td>0.565685</td>\n","      <td>0.160956</td>\n","      <td>0.144985</td>\n","      <td>1.208859</td>\n","      <td>0.100000</td>\n","      <td>0.565685</td>\n","      <td>0.420432</td>\n","      <td>4.657252</td>\n","      <td>5.514526</td>\n","      <td>5.076391</td>\n","      <td>0.176062</td>\n","      <td>0.277034</td>\n","      <td>5.048762</td>\n","      <td>0.857274</td>\n","      <td>0.156928</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>0.0</td>\n","      <td>0.172832</td>\n","      <td>0.169693</td>\n","      <td>0.166202</td>\n","      <td>0.118818</td>\n","      <td>0.114920</td>\n","      <td>18</td>\n","      <td>26</td>\n","      <td>25</td>\n","      <td>24</td>\n","      <td>14</td>\n","      <td>0.700952</td>\n","      <td>0.533470</td>\n","      <td>0.235892</td>\n","      <td>0.214878</td>\n","      <td>0.209049</td>\n","      <td>25</td>\n","      <td>26</td>\n","      <td>24</td>\n","      <td>18</td>\n","      <td>21</td>\n","      <td>0.098433</td>\n","      <td>0.088219</td>\n","      <td>0.073351</td>\n","      <td>0.068003</td>\n","      <td>0.064772</td>\n","      <td>14</td>\n","      <td>26</td>\n","      <td>13</td>\n","      <td>24</td>\n","      <td>12</td>\n","      <td>0.164950</td>\n","      <td>0.147200</td>\n","      <td>0.112870</td>\n","      <td>0.085499</td>\n","      <td>0.083687</td>\n","      <td>14</td>\n","      <td>13</td>\n","      <td>25</td>\n","      <td>...</td>\n","      <td>3.7</td>\n","      <td>5.3</td>\n","      <td>4.645455</td>\n","      <td>0.296784</td>\n","      <td>-0.382833</td>\n","      <td>4.70</td>\n","      <td>1.6</td>\n","      <td>-0.183788</td>\n","      <td>5.8</td>\n","      <td>10.4</td>\n","      <td>8.558182</td>\n","      <td>0.753434</td>\n","      <td>-0.615809</td>\n","      <td>8.6</td>\n","      <td>4.6</td>\n","      <td>-0.055503</td>\n","      <td>7.856844</td>\n","      <td>11.500435</td>\n","      <td>9.778645</td>\n","      <td>0.628718</td>\n","      <td>-0.043196</td>\n","      <td>9.819878</td>\n","      <td>3.643591</td>\n","      <td>-0.065582</td>\n","      <td>0.0</td>\n","      <td>1.392839</td>\n","      <td>0.299192</td>\n","      <td>0.245199</td>\n","      <td>2.026445</td>\n","      <td>0.282843</td>\n","      <td>1.392839</td>\n","      <td>0.066680</td>\n","      <td>3.992493</td>\n","      <td>5.303772</td>\n","      <td>4.705197</td>\n","      <td>0.269491</td>\n","      <td>-0.293768</td>\n","      <td>4.751842</td>\n","      <td>1.311279</td>\n","      <td>-0.173086</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>14787</th>\n","      <td>17495</td>\n","      <td>NaN</td>\n","      <td>0.045147</td>\n","      <td>0.040977</td>\n","      <td>0.040422</td>\n","      <td>0.040261</td>\n","      <td>0.039312</td>\n","      <td>7</td>\n","      <td>20</td>\n","      <td>6</td>\n","      <td>18</td>\n","      <td>15</td>\n","      <td>0.048378</td>\n","      <td>0.046549</td>\n","      <td>0.038826</td>\n","      <td>0.036409</td>\n","      <td>0.031132</td>\n","      <td>7</td>\n","      <td>6</td>\n","      <td>19</td>\n","      <td>20</td>\n","      <td>5</td>\n","      <td>0.015939</td>\n","      <td>0.013628</td>\n","      <td>0.013101</td>\n","      <td>0.011839</td>\n","      <td>0.011687</td>\n","      <td>7</td>\n","      <td>12</td>\n","      <td>14</td>\n","      <td>20</td>\n","      <td>19</td>\n","      <td>0.004613</td>\n","      <td>0.003674</td>\n","      <td>0.003630</td>\n","      <td>0.003618</td>\n","      <td>0.003534</td>\n","      <td>7</td>\n","      <td>20</td>\n","      <td>19</td>\n","      <td>...</td>\n","      <td>5.4</td>\n","      <td>5.8</td>\n","      <td>5.584483</td>\n","      <td>0.110504</td>\n","      <td>-0.005140</td>\n","      <td>5.60</td>\n","      <td>0.4</td>\n","      <td>-0.140422</td>\n","      <td>7.4</td>\n","      <td>8.2</td>\n","      <td>7.756897</td>\n","      <td>0.183640</td>\n","      <td>0.194264</td>\n","      <td>7.7</td>\n","      <td>0.8</td>\n","      <td>0.309827</td>\n","      <td>9.276314</td>\n","      <td>9.912618</td>\n","      <td>9.586268</td>\n","      <td>0.151631</td>\n","      <td>0.119090</td>\n","      <td>9.596874</td>\n","      <td>0.636304</td>\n","      <td>-0.069945</td>\n","      <td>0.0</td>\n","      <td>0.300000</td>\n","      <td>0.052131</td>\n","      <td>0.087095</td>\n","      <td>1.632115</td>\n","      <td>0.000000</td>\n","      <td>0.300000</td>\n","      <td>0.598555</td>\n","      <td>5.423099</td>\n","      <td>5.854912</td>\n","      <td>5.630619</td>\n","      <td>0.113603</td>\n","      <td>-0.093399</td>\n","      <td>5.643580</td>\n","      <td>0.431814</td>\n","      <td>-0.114095</td>\n","    </tr>\n","    <tr>\n","      <th>14788</th>\n","      <td>17496</td>\n","      <td>NaN</td>\n","      <td>0.106112</td>\n","      <td>0.080633</td>\n","      <td>0.066384</td>\n","      <td>0.066344</td>\n","      <td>0.064810</td>\n","      <td>17</td>\n","      <td>18</td>\n","      <td>28</td>\n","      <td>14</td>\n","      <td>6</td>\n","      <td>0.060048</td>\n","      <td>0.055085</td>\n","      <td>0.053703</td>\n","      <td>0.050741</td>\n","      <td>0.039529</td>\n","      <td>17</td>\n","      <td>18</td>\n","      <td>22</td>\n","      <td>28</td>\n","      <td>21</td>\n","      <td>0.054676</td>\n","      <td>0.052369</td>\n","      <td>0.048163</td>\n","      <td>0.038902</td>\n","      <td>0.036345</td>\n","      <td>20</td>\n","      <td>24</td>\n","      <td>27</td>\n","      <td>26</td>\n","      <td>17</td>\n","      <td>0.024449</td>\n","      <td>0.023786</td>\n","      <td>0.007449</td>\n","      <td>0.007021</td>\n","      <td>0.006696</td>\n","      <td>20</td>\n","      <td>24</td>\n","      <td>25</td>\n","      <td>...</td>\n","      <td>5.4</td>\n","      <td>6.0</td>\n","      <td>5.693220</td>\n","      <td>0.122986</td>\n","      <td>0.075420</td>\n","      <td>5.70</td>\n","      <td>0.6</td>\n","      <td>-0.055125</td>\n","      <td>7.1</td>\n","      <td>8.4</td>\n","      <td>7.830508</td>\n","      <td>0.242298</td>\n","      <td>-0.054358</td>\n","      <td>7.8</td>\n","      <td>1.3</td>\n","      <td>0.125913</td>\n","      <td>9.285473</td>\n","      <td>10.261579</td>\n","      <td>9.775604</td>\n","      <td>0.171509</td>\n","      <td>0.064170</td>\n","      <td>9.761660</td>\n","      <td>0.976106</td>\n","      <td>0.081305</td>\n","      <td>0.0</td>\n","      <td>0.509902</td>\n","      <td>0.147790</td>\n","      <td>0.135909</td>\n","      <td>0.553309</td>\n","      <td>0.100000</td>\n","      <td>0.509902</td>\n","      <td>0.351630</td>\n","      <td>5.554278</td>\n","      <td>6.118823</td>\n","      <td>5.848321</td>\n","      <td>0.119011</td>\n","      <td>-0.083291</td>\n","      <td>5.846366</td>\n","      <td>0.564546</td>\n","      <td>0.016422</td>\n","    </tr>\n","    <tr>\n","      <th>14789</th>\n","      <td>17497</td>\n","      <td>NaN</td>\n","      <td>0.197510</td>\n","      <td>0.182615</td>\n","      <td>0.160584</td>\n","      <td>0.148469</td>\n","      <td>0.134668</td>\n","      <td>16</td>\n","      <td>15</td>\n","      <td>13</td>\n","      <td>17</td>\n","      <td>12</td>\n","      <td>0.336796</td>\n","      <td>0.267417</td>\n","      <td>0.235563</td>\n","      <td>0.160510</td>\n","      <td>0.129293</td>\n","      <td>15</td>\n","      <td>13</td>\n","      <td>12</td>\n","      <td>18</td>\n","      <td>17</td>\n","      <td>0.246572</td>\n","      <td>0.192428</td>\n","      <td>0.140895</td>\n","      <td>0.135017</td>\n","      <td>0.117888</td>\n","      <td>13</td>\n","      <td>22</td>\n","      <td>14</td>\n","      <td>17</td>\n","      <td>9</td>\n","      <td>0.512874</td>\n","      <td>0.382228</td>\n","      <td>0.221015</td>\n","      <td>0.173679</td>\n","      <td>0.106141</td>\n","      <td>13</td>\n","      <td>14</td>\n","      <td>7</td>\n","      <td>...</td>\n","      <td>5.3</td>\n","      <td>6.9</td>\n","      <td>6.093103</td>\n","      <td>0.426681</td>\n","      <td>0.029642</td>\n","      <td>6.10</td>\n","      <td>1.6</td>\n","      <td>-0.016163</td>\n","      <td>5.6</td>\n","      <td>9.1</td>\n","      <td>7.427586</td>\n","      <td>0.683334</td>\n","      <td>0.212985</td>\n","      <td>7.4</td>\n","      <td>3.5</td>\n","      <td>0.040370</td>\n","      <td>8.173738</td>\n","      <td>10.745697</td>\n","      <td>9.636704</td>\n","      <td>0.527621</td>\n","      <td>-0.108426</td>\n","      <td>9.670832</td>\n","      <td>2.571958</td>\n","      <td>-0.064683</td>\n","      <td>0.0</td>\n","      <td>1.303840</td>\n","      <td>0.449790</td>\n","      <td>0.253604</td>\n","      <td>1.123254</td>\n","      <td>0.412311</td>\n","      <td>1.303840</td>\n","      <td>0.147787</td>\n","      <td>5.303772</td>\n","      <td>6.987131</td>\n","      <td>6.110052</td>\n","      <td>0.426768</td>\n","      <td>0.042748</td>\n","      <td>6.105325</td>\n","      <td>1.683359</td>\n","      <td>0.011075</td>\n","    </tr>\n","    <tr>\n","      <th>14790</th>\n","      <td>17498</td>\n","      <td>NaN</td>\n","      <td>0.302035</td>\n","      <td>0.207664</td>\n","      <td>0.180798</td>\n","      <td>0.121870</td>\n","      <td>0.112926</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>6</td>\n","      <td>3</td>\n","      <td>9</td>\n","      <td>0.861793</td>\n","      <td>0.640440</td>\n","      <td>0.519377</td>\n","      <td>0.420560</td>\n","      <td>0.329365</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>0.403140</td>\n","      <td>0.097188</td>\n","      <td>0.092168</td>\n","      <td>0.091762</td>\n","      <td>0.084397</td>\n","      <td>4</td>\n","      <td>6</td>\n","      <td>3</td>\n","      <td>19</td>\n","      <td>11</td>\n","      <td>1.517809</td>\n","      <td>0.495129</td>\n","      <td>0.191386</td>\n","      <td>0.076149</td>\n","      <td>0.058186</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>5</td>\n","      <td>...</td>\n","      <td>0.4</td>\n","      <td>2.2</td>\n","      <td>1.275000</td>\n","      <td>0.446094</td>\n","      <td>-0.031063</td>\n","      <td>1.25</td>\n","      <td>1.8</td>\n","      <td>0.056042</td>\n","      <td>8.6</td>\n","      <td>10.2</td>\n","      <td>9.492857</td>\n","      <td>0.351028</td>\n","      <td>-0.249675</td>\n","      <td>9.5</td>\n","      <td>1.6</td>\n","      <td>-0.020348</td>\n","      <td>8.858894</td>\n","      <td>10.321822</td>\n","      <td>9.653737</td>\n","      <td>0.326990</td>\n","      <td>-0.289059</td>\n","      <td>9.671866</td>\n","      <td>1.462928</td>\n","      <td>-0.055445</td>\n","      <td>0.0</td>\n","      <td>1.077033</td>\n","      <td>0.425702</td>\n","      <td>0.270405</td>\n","      <td>0.585549</td>\n","      <td>0.400000</td>\n","      <td>1.077033</td>\n","      <td>0.095051</td>\n","      <td>0.632456</td>\n","      <td>2.662705</td>\n","      <td>1.662364</td>\n","      <td>0.553444</td>\n","      <td>-0.089851</td>\n","      <td>1.752857</td>\n","      <td>2.030250</td>\n","      <td>-0.163509</td>\n","    </tr>\n","    <tr>\n","      <th>14791</th>\n","      <td>17499</td>\n","      <td>NaN</td>\n","      <td>0.166368</td>\n","      <td>0.159512</td>\n","      <td>0.155879</td>\n","      <td>0.131807</td>\n","      <td>0.094000</td>\n","      <td>25</td>\n","      <td>22</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>11</td>\n","      <td>0.343843</td>\n","      <td>0.243732</td>\n","      <td>0.231384</td>\n","      <td>0.229599</td>\n","      <td>0.212400</td>\n","      <td>25</td>\n","      <td>5</td>\n","      <td>22</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>0.102690</td>\n","      <td>0.088671</td>\n","      <td>0.084296</td>\n","      <td>0.084067</td>\n","      <td>0.076163</td>\n","      <td>8</td>\n","      <td>12</td>\n","      <td>11</td>\n","      <td>19</td>\n","      <td>7</td>\n","      <td>0.163182</td>\n","      <td>0.147840</td>\n","      <td>0.115579</td>\n","      <td>0.109928</td>\n","      <td>0.100021</td>\n","      <td>11</td>\n","      <td>8</td>\n","      <td>10</td>\n","      <td>...</td>\n","      <td>1.4</td>\n","      <td>3.2</td>\n","      <td>2.250000</td>\n","      <td>0.353682</td>\n","      <td>0.103562</td>\n","      <td>2.30</td>\n","      <td>1.8</td>\n","      <td>-0.141370</td>\n","      <td>7.7</td>\n","      <td>11.9</td>\n","      <td>9.321429</td>\n","      <td>0.592595</td>\n","      <td>0.946455</td>\n","      <td>9.3</td>\n","      <td>4.2</td>\n","      <td>0.036161</td>\n","      <td>8.734415</td>\n","      <td>12.088424</td>\n","      <td>9.826829</td>\n","      <td>0.482642</td>\n","      <td>1.583826</td>\n","      <td>9.786726</td>\n","      <td>3.354010</td>\n","      <td>0.083092</td>\n","      <td>0.0</td>\n","      <td>1.972308</td>\n","      <td>0.246220</td>\n","      <td>0.305258</td>\n","      <td>3.549861</td>\n","      <td>0.200000</td>\n","      <td>1.972308</td>\n","      <td>0.151414</td>\n","      <td>1.612452</td>\n","      <td>4.860041</td>\n","      <td>3.017111</td>\n","      <td>0.683232</td>\n","      <td>0.358050</td>\n","      <td>3.044666</td>\n","      <td>3.247590</td>\n","      <td>-0.040330</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>14792 rows × 283 columns</p>\n","</div>"],"text/plain":["       fragment_id  behavior_id  ...  fragment_id_xy_g_gap  fragment_id_xy_g_skew2\n","0                0          0.0  ...              0.760028               -0.002016\n","1                1          0.0  ...              0.545005                0.227288\n","2                2          0.0  ...              1.026763                0.379691\n","3                3          0.0  ...              0.857274                0.156928\n","4                4          0.0  ...              1.311279               -0.173086\n","...            ...          ...  ...                   ...                     ...\n","14787        17495          NaN  ...              0.431814               -0.114095\n","14788        17496          NaN  ...              0.564546                0.016422\n","14789        17497          NaN  ...              1.683359                0.011075\n","14790        17498          NaN  ...              2.030250               -0.163509\n","14791        17499          NaN  ...              3.247590               -0.040330\n","\n","[14792 rows x 283 columns]"]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"markdown","metadata":{"id":"ZRND6maa8HYZ","colab_type":"text","pycharm":{}},"source":["# 数据预处理"]},{"cell_type":"code","metadata":{"id":"zVY7p2QA6pv1","colab_type":"code","pycharm":{},"colab":{}},"source":["label_feat = 'behavior_id'\n","train_df = df_tr_te[((df_tr_te[label_feat].isna()==False) & (df_tr_te[label_feat] >=0))].reset_index(drop=True)\n","test_df  = df_tr_te[((df_tr_te[label_feat].isna()==True) | (df_tr_te[label_feat] < 0))].reset_index(drop=True)\n","\n","\n","# 根据lightgbm树模型得到的特征\n","selected_feat = ['fragment_id_acc_yg_max',\n"," 'fragment_id_xy_median',\n"," 'fragment_id_acc_yg_min',\n"," 'acc_xg_fftA_0',\n"," \n"," 'acc_yg_fftA_0',\n"," 'accg_fftA_0',\n"," 'fragment_id_acc_yg_gap',\n"," 'fragment_id_xy_g_max',\n"," 'fragment_id_acc_xg_mean',\n"," 'fragment_id_acc_xg_max',\n"," 'cnt',\n"," 'acc_y_fftA_0',\n"," 'fragment_id_acc_yg_mean',\n"," 'fragment_id_acc_yg_std',\n"," 'fragment_id_acc_yg_median',\n"," 'acc_zg_fftA_0',\n"," 'fragment_id_acc_zg_min',\n"," 'fragment_id_accg_median',\n"," 'fragment_id_acc_xg_min',\n"," 'fragment_id_acc_xg_median']\n","train_stat = train_df[selected_feat]\n","test_stat  = test_df[selected_feat]\n","\n","train_stat = np.array(train_stat.values)\n","test_stat = np.array(test_stat.values)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SgBmaGg57pox","colab_type":"text","pycharm":{}},"source":["# 标准化"]},{"cell_type":"code","metadata":{"id":"OIXbRRU07sKn","colab_type":"code","pycharm":{},"colab":{}},"source":["def autos(X):\n","    m, n = X.shape[0], X.shape[1] \n","    mu = np.mean(X, axis=0)\n","    sigma = np.std(X, axis=0, ddof=1)\n","    X_ = ((X - mu) / (sigma))\n","    return X_\n","train_stat = autos(train_stat)\n","test_stat = autos(test_stat)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cJPy_xtd7o8R","colab_type":"code","pycharm":{},"colab":{"base_uri":"https://localhost:8080/","height":124},"executionInfo":{"status":"ok","timestamp":1596102198393,"user_tz":-480,"elapsed":389903,"user":{"displayName":"guang wayne","photoUrl":"","userId":"12121093145507417035"}},"outputId":"e89107b1-a8fd-41f2-b96a-68748501f6d9"},"source":["def standardization(X):\n","    # x1 = X.transpose(0, 1, 3, 2)\n","    x1 = X\n","    x2 = x1.reshape(-1, x1.shape[-2])\n","    # mean = [8.03889039e-03, -6.41381949e-02, 2.37856977e-02, 8.64949391e-01,\n","    #         2.80964889e+00, 7.83041714e+00, 6.44853358e-01, 9.78580749e+00]\n","    # std = [0.6120893, 0.53693888, 0.7116134, 3.22046385, 3.01195336, 2.61300056, 0.87194132, 0.68427254]\n","    mu=np.mean(x2,axis=0)\n","    sigma=np.std(x2,axis=0)\n","    print(mu,sigma)\n","    x3 = ((x2 - mu) / (sigma))\n","    # x4 = x3.reshape(x1.shape).transpose(0, 1, 3, 2)\n","    x4 = x3.reshape(x1.shape)\n","    return x4\n","\n","x = standardization(x)\n","t = standardization(t)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[ 9.75304151e-03 -6.36132955e-02  2.48834835e-02  9.64539739e-01\n","  2.73127545e+00  7.82182825e+00  6.50165171e-01  9.79294912e+00] [0.62101194 0.53955464 0.71851487 3.29887193 2.98092609 2.65323979\n"," 0.88162122 0.69216192]\n","[ 6.37227840e-03 -6.46485372e-02  2.27183572e-02  7.68121016e-01\n","  2.88584878e+00  7.83876784e+00  6.39688859e-01  9.77886392e+00] [0.60328295 0.53438288 0.7048369  3.13932736 3.03988    2.57324659\n"," 0.86239434 0.67644147]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"d5Z6kdZgo3DG","colab_type":"text"},"source":["# 数据增强"]},{"cell_type":"code","metadata":{"id":"6GFp08Ktq_BG","colab_type":"code","colab":{}},"source":["def jitter(x, snr_db):\n","    \"\"\"\n","    根据信噪比添加噪声\n","    :param x:\n","    :param snr_db:\n","    :return:\n","    \"\"\"\n","    # 随机选择信噪比\n","    assert isinstance(snr_db, list)\n","    snr_db_low = snr_db[0]\n","    snr_db_up = snr_db[1]\n","    snr_db = np.random.randint(snr_db_low, snr_db_up, (1,))[0]\n","\n","    snr = 10 ** (snr_db / 10)\n","    Xp = np.sum(x ** 2, axis=0, keepdims=True) / x.shape[0]  # 计算信号功率\n","    Np = Xp / snr  # 计算噪声功率\n","    n = np.random.normal(size=x.shape, scale=np.sqrt(Np), loc=0.0)  # 计算噪声 loc均值，scale方差\n","    xn = x + n\n","    return xn"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NX65weWmpGyy","colab_type":"code","colab":{}},"source":["x1 = jitter(x,[5,15])\n","x = np.concatenate([x, x1], axis=0)\n","y = np.concatenate([y, y], axis=0)\n","train_stat = np.concatenate([train_stat,train_stat],axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8yyTeFzMJoHj","colab_type":"code","pycharm":{},"colab":{}},"source":["from keras.preprocessing.image import ImageDataGenerator\n","datagen = ImageDataGenerator(zoom_range = 0,\n","                            height_shift_range = 0.2,\n","                            width_shift_range = 0,\n","                            rotation_range = 0)\n","\n","# Here is the function that merges our two generators\n","# We use the exact same generator with the same random seed for both the y and angle arrays\n","def gen_flow_for_two_inputs(X1, X2, y, batch_size):\n","    genX1 = datagen.flow(X1,y,  batch_size=batch_size,seed = 2020)\n","    genX2 = datagen.flow(X1,X2, batch_size=\n","                         batch_size,seed = 2020)\n","    while True:\n","            X1i = genX1.next()\n","            X2i = genX2.next()\n","            #Assert arrays are equal - this was for peace of mind, but slows down training\n","            #np.testing.assert_array_equal(X1i[0],X2i[0])\n","            yield [X1i[0], X2i[1]], X1i[1]\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ro58gTbTsk9c","colab_type":"text"},"source":["# 根据官方评分函数改写的tf评价函数"]},{"cell_type":"code","metadata":{"id":"RToZHGQTsjp6","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","def get_acc_combo():\n","    def combo(y, y_pred):\n","        # 数值ID与行为编码的对应关系\n","        mapping = {0: 'A_0', 1: 'A_1', 2: 'A_2', 3: 'A_3',\n","            4: 'D_4', 5: 'A_5', 6: 'B_1',7: 'B_5',\n","            8: 'B_2', 9: 'B_3', 10: 'B_0', 11: 'A_6',\n","            12: 'C_1', 13: 'C_3', 14: 'C_0', 15: 'B_6',\n","            16: 'C_2', 17: 'C_5', 18: 'C_6'}\n","        # 将行为ID转为编码\n","\n","        code_y, code_y_pred = mapping[int(y)], mapping[int(y_pred)]\n","        if code_y == code_y_pred: #编码完全相同得分1.0\n","            return 1.0\n","        elif code_y.split(\"_\")[0] == code_y_pred.split(\"_\")[0]: #编码仅字母部分相同得分1.0/7\n","            return 1.0/7\n","        elif code_y.split(\"_\")[1] == code_y_pred.split(\"_\")[1]: #编码仅数字部分相同得分1.0/3\n","            return 1.0/3\n","        else:\n","            return 0.0\n","\n","    confusionMatrix=np.zeros((19,19))\n","    for i in range(19):\n","        for j in range(19):\n","            confusionMatrix[i,j]=combo(i,j)\n","    confusionMatrix=tf.convert_to_tensor(confusionMatrix)\n","\n","    def acc_combo(y, y_pred):\n","        y=tf.argmax(y,axis=1)\n","        y_pred = tf.argmax(y_pred, axis=1)\n","        indices=tf.stack([y,y_pred],axis=1)#在1轴增加一个维度\n","        scores=tf.gather_nd(confusionMatrix,tf.cast(indices,tf.int32))\n","        return tf.reduce_mean(scores)\n","    return acc_combo"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U04R-M-RKY5B","colab_type":"code","pycharm":{},"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1596105960111,"user_tz":-480,"elapsed":463998,"user":{"displayName":"guang wayne","photoUrl":"","userId":"12121093145507417035"}},"outputId":"69cf3846-d384-40f5-f6ff-47526c611722"},"source":["kfold = StratifiedKFold(5, shuffle=True, random_state=2020)\n","def Net():\n","    input = Input(shape=(60, 8, 1))\n","    hin = Input(shape=(20, ))\n","    X = Conv2D(filters= 64,\n","               kernel_size=(3, 3),\n","               activation='relu',\n","               padding='same')(input)\n","    X = BatchNormalization()(X)\n","    X = Conv2D(filters= 128,\n","               kernel_size=(3, 3),\n","               activation='relu',\n","               padding='same')(X)\n","    X = BatchNormalization()(X)\n","    X = MaxPooling2D()(X)\n","    X = Conv2D(filters=256,\n","               kernel_size=(3, 3),\n","               activation='relu',\n","               padding='same')(X)\n","    X = BatchNormalization()(X)\n","    X = Conv2D(filters=512,\n","               kernel_size=(3, 3),\n","               activation='relu',\n","               padding='same')(X)\n","    X = BatchNormalization()(X)\n","    X = GlobalMaxPooling2D()(X)\n","    merge = concatenate([X, hin])\n","    merge = BatchNormalization()(merge)\n","    merge = Dropout(0.3)(merge)\n","    X = Dense(64,activation='relu')(merge)\n","    X = BatchNormalization()(X)\n","\n","    y = Dense(19, activation='softmax')(X)\n","    return Model(inputs=[input, hin], outputs=y)\n","\n","\n","proba_t = np.zeros((7500, 19))\n","val_loss = []\n","val_acc = []\n","batch_size = 64\n","for fold, (xx, yy) in enumerate(kfold.split(x, y)):\n","    y_ = to_categorical(y, num_classes=19)\n","    model = Net()\n","    print(model.summary())\n","    model.compile(loss='categorical_crossentropy',\n","                  optimizer=Adam(5e-3),\n","                  metrics=['acc',get_acc_combo()])\n","    plateau = ReduceLROnPlateau(monitor=\"val_acc_combo\",\n","                                verbose=1,\n","                                mode='max',\n","                                factor=0.5,\n","                                patience=4)\n","    early_stopping = EarlyStopping(monitor='val_acc_combo',\n","                                   verbose=1,\n","                                   mode='max',\n","                                   patience=20)\n","    checkpoint = ModelCheckpoint(f'data/fold{fold}.h5',\n","                                 monitor='val_acc_combo',\n","                                 verbose=1,\n","                                 mode='max',\n","                                 save_best_only=True)\n","    #gen_flow = gen_flow_for_two_inputs(x[xx], train_stat[xx], y_[xx], batch_size)\n","    hist =  model.fit([x[xx],train_stat[xx]], y_[xx],\n","              #gen_flow,steps_per_epoch=x[xx].shape[0] / batch_size,\n","              epochs=100,\n","              verbose=1,\n","              shuffle=True,\n","              validation_data=([x[yy],train_stat[yy]], y_[yy]),\n","              callbacks=[plateau, early_stopping, checkpoint])\n","    val_loss.append(np.min(hist.history['val_loss']))\n","    val_acc.append(np.max(hist.history['val_acc']))\n","    #model.load_weights(f'data/fold{fold}.h5')\n","    proba_t += model.predict([t,test_stat], verbose=0, batch_size=1024) / 5.\n","print('log loss:', np.mean(val_loss))\n","print('val_acc:', np.mean(val_acc))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"model_12\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_25 (InputLayer)           [(None, 60, 8, 1)]   0                                            \n","__________________________________________________________________________________________________\n","conv2d_48 (Conv2D)              (None, 60, 8, 64)    640         input_25[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_72 (BatchNo (None, 60, 8, 64)    256         conv2d_48[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_49 (Conv2D)              (None, 60, 8, 128)   73856       batch_normalization_72[0][0]     \n","__________________________________________________________________________________________________\n","batch_normalization_73 (BatchNo (None, 60, 8, 128)   512         conv2d_49[0][0]                  \n","__________________________________________________________________________________________________\n","max_pooling2d_12 (MaxPooling2D) (None, 30, 4, 128)   0           batch_normalization_73[0][0]     \n","__________________________________________________________________________________________________\n","conv2d_50 (Conv2D)              (None, 30, 4, 256)   295168      max_pooling2d_12[0][0]           \n","__________________________________________________________________________________________________\n","batch_normalization_74 (BatchNo (None, 30, 4, 256)   1024        conv2d_50[0][0]                  \n","__________________________________________________________________________________________________\n","conv2d_51 (Conv2D)              (None, 30, 4, 512)   1180160     batch_normalization_74[0][0]     \n","__________________________________________________________________________________________________\n","batch_normalization_75 (BatchNo (None, 30, 4, 512)   2048        conv2d_51[0][0]                  \n","__________________________________________________________________________________________________\n","global_max_pooling2d_12 (Global (None, 512)          0           batch_normalization_75[0][0]     \n","__________________________________________________________________________________________________\n","input_26 (InputLayer)           [(None, 20)]         0                                            \n","__________________________________________________________________________________________________\n","concatenate_12 (Concatenate)    (None, 532)          0           global_max_pooling2d_12[0][0]    \n","                                                                 input_26[0][0]                   \n","__________________________________________________________________________________________________\n","batch_normalization_76 (BatchNo (None, 532)          2128        concatenate_12[0][0]             \n","__________________________________________________________________________________________________\n","dropout_12 (Dropout)            (None, 532)          0           batch_normalization_76[0][0]     \n","__________________________________________________________________________________________________\n","dense_24 (Dense)                (None, 64)           34112       dropout_12[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_77 (BatchNo (None, 64)           256         dense_24[0][0]                   \n","__________________________________________________________________________________________________\n","dense_25 (Dense)                (None, 19)           1235        batch_normalization_77[0][0]     \n","==================================================================================================\n","Total params: 1,591,395\n","Trainable params: 1,588,283\n","Non-trainable params: 3,112\n","__________________________________________________________________________________________________\n","None\n","Epoch 1/100\n","363/365 [============================>.] - ETA: 0s - loss: 1.7257 - acc: 0.4102 - acc_combo: 0.4917\n","Epoch 00001: val_acc_combo improved from -inf to 0.51328, saving model to data/fold0.h5\n","365/365 [==============================] - 11s 29ms/step - loss: 1.7248 - acc: 0.4106 - acc_combo: 0.4920 - val_loss: 1.6813 - val_acc: 0.4381 - val_acc_combo: 0.5133 - lr: 0.0050\n","Epoch 2/100\n","363/365 [============================>.] - ETA: 0s - loss: 1.3546 - acc: 0.5248 - acc_combo: 0.5924\n","Epoch 00002: val_acc_combo improved from 0.51328 to 0.62814, saving model to data/fold0.h5\n","365/365 [==============================] - 10s 26ms/step - loss: 1.3540 - acc: 0.5249 - acc_combo: 0.5926 - val_loss: 1.1959 - val_acc: 0.5612 - val_acc_combo: 0.6281 - lr: 0.0050\n","Epoch 3/100\n","363/365 [============================>.] - ETA: 0s - loss: 1.2106 - acc: 0.5665 - acc_combo: 0.6300\n","Epoch 00003: val_acc_combo improved from 0.62814 to 0.66579, saving model to data/fold0.h5\n","365/365 [==============================] - 9s 26ms/step - loss: 1.2096 - acc: 0.5671 - acc_combo: 0.6305 - val_loss: 1.1110 - val_acc: 0.6061 - val_acc_combo: 0.6658 - lr: 0.0050\n","Epoch 4/100\n","364/365 [============================>.] - ETA: 0s - loss: 1.0631 - acc: 0.6150 - acc_combo: 0.6726\n","Epoch 00004: val_acc_combo improved from 0.66579 to 0.69575, saving model to data/fold0.h5\n","365/365 [==============================] - 10s 28ms/step - loss: 1.0633 - acc: 0.6150 - acc_combo: 0.6725 - val_loss: 1.0070 - val_acc: 0.6435 - val_acc_combo: 0.6957 - lr: 0.0050\n","Epoch 5/100\n","365/365 [==============================] - ETA: 0s - loss: 0.9479 - acc: 0.6581 - acc_combo: 0.7094\n","Epoch 00005: val_acc_combo improved from 0.69575 to 0.71553, saving model to data/fold0.h5\n","365/365 [==============================] - 10s 26ms/step - loss: 0.9479 - acc: 0.6581 - acc_combo: 0.7094 - val_loss: 0.9265 - val_acc: 0.6647 - val_acc_combo: 0.7155 - lr: 0.0050\n","Epoch 6/100\n","364/365 [============================>.] - ETA: 0s - loss: 0.8454 - acc: 0.6947 - acc_combo: 0.7408\n","Epoch 00006: val_acc_combo improved from 0.71553 to 0.73701, saving model to data/fold0.h5\n","365/365 [==============================] - 10s 29ms/step - loss: 0.8464 - acc: 0.6945 - acc_combo: 0.7405 - val_loss: 0.8476 - val_acc: 0.6870 - val_acc_combo: 0.7370 - lr: 0.0050\n","Epoch 7/100\n","365/365 [==============================] - ETA: 0s - loss: 0.7596 - acc: 0.7278 - acc_combo: 0.7684\n","Epoch 00007: val_acc_combo improved from 0.73701 to 0.74833, saving model to data/fold0.h5\n","365/365 [==============================] - 10s 26ms/step - loss: 0.7596 - acc: 0.7278 - acc_combo: 0.7684 - val_loss: 0.8360 - val_acc: 0.7017 - val_acc_combo: 0.7483 - lr: 0.0050\n","Epoch 8/100\n","363/365 [============================>.] - ETA: 0s - loss: 0.6720 - acc: 0.7548 - acc_combo: 0.7916\n","Epoch 00008: val_acc_combo did not improve from 0.74833\n","365/365 [==============================] - 9s 26ms/step - loss: 0.6733 - acc: 0.7543 - acc_combo: 0.7908 - val_loss: 0.9047 - val_acc: 0.6891 - val_acc_combo: 0.7347 - lr: 0.0050\n","Epoch 9/100\n","364/365 [============================>.] - ETA: 0s - loss: 0.6103 - acc: 0.7813 - acc_combo: 0.8156\n","Epoch 00009: val_acc_combo improved from 0.74833 to 0.76820, saving model to data/fold0.h5\n","365/365 [==============================] - 10s 28ms/step - loss: 0.6104 - acc: 0.7814 - acc_combo: 0.8158 - val_loss: 0.7746 - val_acc: 0.7223 - val_acc_combo: 0.7682 - lr: 0.0050\n","Epoch 10/100\n","363/365 [============================>.] - ETA: 0s - loss: 0.5453 - acc: 0.8026 - acc_combo: 0.8327\n","Epoch 00010: val_acc_combo improved from 0.76820 to 0.81413, saving model to data/fold0.h5\n","365/365 [==============================] - 10s 26ms/step - loss: 0.5455 - acc: 0.8025 - acc_combo: 0.8327 - val_loss: 0.6276 - val_acc: 0.7768 - val_acc_combo: 0.8141 - lr: 0.0050\n","Epoch 11/100\n","365/365 [==============================] - ETA: 0s - loss: 0.4929 - acc: 0.8197 - acc_combo: 0.8473\n","Epoch 00011: val_acc_combo did not improve from 0.81413\n","365/365 [==============================] - 9s 25ms/step - loss: 0.4929 - acc: 0.8197 - acc_combo: 0.8473 - val_loss: 0.6573 - val_acc: 0.7672 - val_acc_combo: 0.8057 - lr: 0.0050\n","Epoch 12/100\n","364/365 [============================>.] - ETA: 0s - loss: 0.4508 - acc: 0.8383 - acc_combo: 0.8634\n","Epoch 00012: val_acc_combo did not improve from 0.81413\n","365/365 [==============================] - 9s 25ms/step - loss: 0.4515 - acc: 0.8381 - acc_combo: 0.8631 - val_loss: 0.7754 - val_acc: 0.7535 - val_acc_combo: 0.7946 - lr: 0.0050\n","Epoch 13/100\n","363/365 [============================>.] - ETA: 0s - loss: 0.3925 - acc: 0.8611 - acc_combo: 0.8827\n","Epoch 00013: val_acc_combo improved from 0.81413 to 0.81916, saving model to data/fold0.h5\n","365/365 [==============================] - 10s 28ms/step - loss: 0.3923 - acc: 0.8611 - acc_combo: 0.8826 - val_loss: 0.6786 - val_acc: 0.7885 - val_acc_combo: 0.8192 - lr: 0.0050\n","Epoch 14/100\n","365/365 [==============================] - ETA: 0s - loss: 0.3657 - acc: 0.8698 - acc_combo: 0.8898\n","Epoch 00014: val_acc_combo improved from 0.81916 to 0.82882, saving model to data/fold0.h5\n","365/365 [==============================] - 10s 28ms/step - loss: 0.3657 - acc: 0.8698 - acc_combo: 0.8898 - val_loss: 0.6436 - val_acc: 0.7981 - val_acc_combo: 0.8288 - lr: 0.0050\n","Epoch 15/100\n","365/365 [==============================] - ETA: 0s - loss: 0.3203 - acc: 0.8853 - acc_combo: 0.9036\n","Epoch 00015: val_acc_combo improved from 0.82882 to 0.82948, saving model to data/fold0.h5\n","365/365 [==============================] - 9s 26ms/step - loss: 0.3203 - acc: 0.8853 - acc_combo: 0.9036 - val_loss: 0.6414 - val_acc: 0.7977 - val_acc_combo: 0.8295 - lr: 0.0050\n","Epoch 16/100\n","364/365 [============================>.] - ETA: 0s - loss: 0.2958 - acc: 0.8937 - acc_combo: 0.9106\n","Epoch 00016: val_acc_combo improved from 0.82948 to 0.84482, saving model to data/fold0.h5\n","365/365 [==============================] - 10s 28ms/step - loss: 0.2956 - acc: 0.8938 - acc_combo: 0.9108 - val_loss: 0.5576 - val_acc: 0.8159 - val_acc_combo: 0.8448 - lr: 0.0050\n","Epoch 17/100\n","364/365 [============================>.] - ETA: 0s - loss: 0.2777 - acc: 0.9023 - acc_combo: 0.9172\n","Epoch 00017: val_acc_combo improved from 0.84482 to 0.84663, saving model to data/fold0.h5\n","365/365 [==============================] - 10s 28ms/step - loss: 0.2777 - acc: 0.9023 - acc_combo: 0.9172 - val_loss: 0.5894 - val_acc: 0.8207 - val_acc_combo: 0.8466 - lr: 0.0050\n","Epoch 18/100\n","364/365 [============================>.] - ETA: 0s - loss: 0.2580 - acc: 0.9059 - acc_combo: 0.9204\n","Epoch 00018: val_acc_combo did not improve from 0.84663\n","365/365 [==============================] - 10s 26ms/step - loss: 0.2579 - acc: 0.9059 - acc_combo: 0.9204 - val_loss: 0.6322 - val_acc: 0.8125 - val_acc_combo: 0.8395 - lr: 0.0050\n","Epoch 19/100\n","365/365 [==============================] - ETA: 0s - loss: 0.2315 - acc: 0.9163 - acc_combo: 0.9290\n","Epoch 00019: val_acc_combo did not improve from 0.84663\n","365/365 [==============================] - 9s 26ms/step - loss: 0.2315 - acc: 0.9163 - acc_combo: 0.9290 - val_loss: 0.6159 - val_acc: 0.8046 - val_acc_combo: 0.8327 - lr: 0.0050\n","Epoch 20/100\n","363/365 [============================>.] - ETA: 0s - loss: 0.2151 - acc: 0.9245 - acc_combo: 0.9357\n","Epoch 00020: val_acc_combo did not improve from 0.84663\n","365/365 [==============================] - 9s 25ms/step - loss: 0.2154 - acc: 0.9244 - acc_combo: 0.9355 - val_loss: 0.7201 - val_acc: 0.7912 - val_acc_combo: 0.8260 - lr: 0.0050\n","Epoch 21/100\n","364/365 [============================>.] - ETA: 0s - loss: 0.2002 - acc: 0.9281 - acc_combo: 0.9392\n","Epoch 00021: val_acc_combo improved from 0.84663 to 0.85261, saving model to data/fold0.h5\n","365/365 [==============================] - 10s 28ms/step - loss: 0.2000 - acc: 0.9282 - acc_combo: 0.9392 - val_loss: 0.6132 - val_acc: 0.8265 - val_acc_combo: 0.8526 - lr: 0.0050\n","Epoch 22/100\n","364/365 [============================>.] - ETA: 0s - loss: 0.1849 - acc: 0.9341 - acc_combo: 0.9443\n","Epoch 00022: val_acc_combo did not improve from 0.85261\n","365/365 [==============================] - 9s 25ms/step - loss: 0.1850 - acc: 0.9340 - acc_combo: 0.9442 - val_loss: 0.5888 - val_acc: 0.8224 - val_acc_combo: 0.8504 - lr: 0.0050\n","Epoch 23/100\n","363/365 [============================>.] - ETA: 0s - loss: 0.1819 - acc: 0.9346 - acc_combo: 0.9450\n","Epoch 00023: val_acc_combo did not improve from 0.85261\n","365/365 [==============================] - 9s 25ms/step - loss: 0.1816 - acc: 0.9346 - acc_combo: 0.9450 - val_loss: 0.6037 - val_acc: 0.8258 - val_acc_combo: 0.8516 - lr: 0.0050\n","Epoch 24/100\n","364/365 [============================>.] - ETA: 0s - loss: 0.1549 - acc: 0.9451 - acc_combo: 0.9530\n","Epoch 00024: val_acc_combo did not improve from 0.85261\n","365/365 [==============================] - 9s 25ms/step - loss: 0.1554 - acc: 0.9446 - acc_combo: 0.9523 - val_loss: 0.7494 - val_acc: 0.8022 - val_acc_combo: 0.8348 - lr: 0.0050\n","Epoch 25/100\n","364/365 [============================>.] - ETA: 0s - loss: 0.1574 - acc: 0.9439 - acc_combo: 0.9530\n","Epoch 00025: val_acc_combo improved from 0.85261 to 0.85470, saving model to data/fold0.h5\n","365/365 [==============================] - 10s 28ms/step - loss: 0.1574 - acc: 0.9439 - acc_combo: 0.9528 - val_loss: 0.6082 - val_acc: 0.8276 - val_acc_combo: 0.8547 - lr: 0.0050\n","Epoch 26/100\n","365/365 [==============================] - ETA: 0s - loss: 0.1435 - acc: 0.9494 - acc_combo: 0.9565\n","Epoch 00026: val_acc_combo improved from 0.85470 to 0.85659, saving model to data/fold0.h5\n","365/365 [==============================] - 9s 26ms/step - loss: 0.1435 - acc: 0.9494 - acc_combo: 0.9565 - val_loss: 0.6844 - val_acc: 0.8269 - val_acc_combo: 0.8566 - lr: 0.0050\n","Epoch 27/100\n","365/365 [==============================] - ETA: 0s - loss: 0.1412 - acc: 0.9496 - acc_combo: 0.9569\n","Epoch 00027: val_acc_combo did not improve from 0.85659\n","365/365 [==============================] - 9s 25ms/step - loss: 0.1412 - acc: 0.9496 - acc_combo: 0.9569 - val_loss: 0.6274 - val_acc: 0.8252 - val_acc_combo: 0.8524 - lr: 0.0050\n","Epoch 28/100\n","364/365 [============================>.] - ETA: 0s - loss: 0.1382 - acc: 0.9502 - acc_combo: 0.9583\n","Epoch 00028: val_acc_combo did not improve from 0.85659\n","365/365 [==============================] - 9s 25ms/step - loss: 0.1381 - acc: 0.9502 - acc_combo: 0.9583 - val_loss: 0.6551 - val_acc: 0.8276 - val_acc_combo: 0.8542 - lr: 0.0050\n","Epoch 29/100\n","365/365 [==============================] - ETA: 0s - loss: 0.1356 - acc: 0.9510 - acc_combo: 0.9585\n","Epoch 00029: val_acc_combo did not improve from 0.85659\n","365/365 [==============================] - 9s 26ms/step - loss: 0.1356 - acc: 0.9510 - acc_combo: 0.9585 - val_loss: 0.7662 - val_acc: 0.8210 - val_acc_combo: 0.8508 - lr: 0.0050\n","Epoch 30/100\n","364/365 [============================>.] - ETA: 0s - loss: 0.1200 - acc: 0.9574 - acc_combo: 0.9642\n","Epoch 00030: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n","\n","Epoch 00030: val_acc_combo did not improve from 0.85659\n","365/365 [==============================] - 9s 25ms/step - loss: 0.1202 - acc: 0.9573 - acc_combo: 0.9641 - val_loss: 0.6238 - val_acc: 0.8272 - val_acc_combo: 0.8533 - lr: 0.0050\n","Epoch 31/100\n","363/365 [============================>.] - ETA: 0s - loss: 0.0727 - acc: 0.9744 - acc_combo: 0.9783\n","Epoch 00031: val_acc_combo improved from 0.85659 to 0.87904, saving model to data/fold0.h5\n","365/365 [==============================] - 11s 29ms/step - loss: 0.0726 - acc: 0.9745 - acc_combo: 0.9783 - val_loss: 0.5420 - val_acc: 0.8560 - val_acc_combo: 0.8790 - lr: 0.0025\n","Epoch 32/100\n","364/365 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9855 - acc_combo: 0.9877\n","Epoch 00032: val_acc_combo improved from 0.87904 to 0.88336, saving model to data/fold0.h5\n","365/365 [==============================] - 9s 26ms/step - loss: 0.0470 - acc: 0.9855 - acc_combo: 0.9878 - val_loss: 0.5243 - val_acc: 0.8622 - val_acc_combo: 0.8834 - lr: 0.0025\n","Epoch 33/100\n","364/365 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9887 - acc_combo: 0.9905\n","Epoch 00033: val_acc_combo improved from 0.88336 to 0.89171, saving model to data/fold0.h5\n","365/365 [==============================] - 10s 27ms/step - loss: 0.0347 - acc: 0.9887 - acc_combo: 0.9905 - val_loss: 0.4932 - val_acc: 0.8718 - val_acc_combo: 0.8917 - lr: 0.0025\n","Epoch 34/100\n","363/365 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9886 - acc_combo: 0.9904\n","Epoch 00034: val_acc_combo did not improve from 0.89171\n","365/365 [==============================] - 9s 25ms/step - loss: 0.0344 - acc: 0.9887 - acc_combo: 0.9905 - val_loss: 0.5642 - val_acc: 0.8601 - val_acc_combo: 0.8820 - lr: 0.0025\n","Epoch 35/100\n","364/365 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9898 - acc_combo: 0.9913\n","Epoch 00035: val_acc_combo did not improve from 0.89171\n","365/365 [==============================] - 9s 25ms/step - loss: 0.0330 - acc: 0.9898 - acc_combo: 0.9914 - val_loss: 0.5325 - val_acc: 0.8697 - val_acc_combo: 0.8902 - lr: 0.0025\n","Epoch 36/100\n","365/365 [==============================] - ETA: 0s - loss: 0.0483 - acc: 0.9822 - acc_combo: 0.9845\n","Epoch 00036: val_acc_combo did not improve from 0.89171\n","365/365 [==============================] - 9s 25ms/step - loss: 0.0483 - acc: 0.9822 - acc_combo: 0.9845 - val_loss: 0.6219 - val_acc: 0.8522 - val_acc_combo: 0.8757 - lr: 0.0025\n","Epoch 37/100\n","363/365 [============================>.] - ETA: 0s - loss: 0.0427 - acc: 0.9848 - acc_combo: 0.9873\n","Epoch 00037: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n","\n","Epoch 00037: val_acc_combo did not improve from 0.89171\n","365/365 [==============================] - 9s 25ms/step - loss: 0.0431 - acc: 0.9848 - acc_combo: 0.9872 - val_loss: 0.6898 - val_acc: 0.8389 - val_acc_combo: 0.8634 - lr: 0.0025\n","Epoch 38/100\n","365/365 [==============================] - ETA: 0s - loss: 0.0229 - acc: 0.9931 - acc_combo: 0.9940\n","Epoch 00038: val_acc_combo did not improve from 0.89171\n","365/365 [==============================] - 9s 25ms/step - loss: 0.0229 - acc: 0.9931 - acc_combo: 0.9940 - val_loss: 0.5527 - val_acc: 0.8670 - val_acc_combo: 0.8873 - lr: 0.0012\n","Epoch 39/100\n","365/365 [==============================] - ETA: 0s - loss: 0.0195 - acc: 0.9941 - acc_combo: 0.9951\n","Epoch 00039: val_acc_combo did not improve from 0.89171\n","365/365 [==============================] - 9s 25ms/step - loss: 0.0195 - acc: 0.9941 - acc_combo: 0.9951 - val_loss: 0.5596 - val_acc: 0.8656 - val_acc_combo: 0.8857 - lr: 0.0012\n","Epoch 40/100\n","364/365 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9946 - acc_combo: 0.9954\n","Epoch 00040: val_acc_combo did not improve from 0.89171\n","365/365 [==============================] - 9s 25ms/step - loss: 0.0165 - acc: 0.9946 - acc_combo: 0.9955 - val_loss: 0.5599 - val_acc: 0.8694 - val_acc_combo: 0.8890 - lr: 0.0012\n","Epoch 41/100\n","363/365 [============================>.] - ETA: 0s - loss: 0.0137 - acc: 0.9951 - acc_combo: 0.9959\n","Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n","\n","Epoch 00041: val_acc_combo did not improve from 0.89171\n","365/365 [==============================] - 9s 25ms/step - loss: 0.0137 - acc: 0.9951 - acc_combo: 0.9959 - val_loss: 0.5894 - val_acc: 0.8677 - val_acc_combo: 0.8872 - lr: 0.0012\n","Epoch 42/100\n","363/365 [============================>.] - ETA: 0s - loss: 0.0108 - acc: 0.9972 - acc_combo: 0.9976\n","Epoch 00042: val_acc_combo did not improve from 0.89171\n","365/365 [==============================] - 9s 25ms/step - loss: 0.0107 - acc: 0.9972 - acc_combo: 0.9977 - val_loss: 0.5870 - val_acc: 0.8673 - val_acc_combo: 0.8876 - lr: 6.2500e-04\n","Epoch 43/100\n","363/365 [============================>.] - ETA: 0s - loss: 0.0136 - acc: 0.9955 - acc_combo: 0.9963\n","Epoch 00043: val_acc_combo improved from 0.89171 to 0.89294, saving model to data/fold0.h5\n","365/365 [==============================] - 10s 28ms/step - loss: 0.0136 - acc: 0.9955 - acc_combo: 0.9963 - val_loss: 0.5590 - val_acc: 0.8735 - val_acc_combo: 0.8929 - lr: 6.2500e-04\n","Epoch 44/100\n","363/365 [============================>.] - ETA: 0s - loss: 0.0126 - acc: 0.9958 - acc_combo: 0.9964\n","Epoch 00044: val_acc_combo did not improve from 0.89294\n","365/365 [==============================] - 9s 25ms/step - loss: 0.0125 - acc: 0.9958 - acc_combo: 0.9964 - val_loss: 0.5647 - val_acc: 0.8708 - val_acc_combo: 0.8910 - lr: 6.2500e-04\n","Epoch 45/100\n","364/365 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9968 - acc_combo: 0.9973\n","Epoch 00045: val_acc_combo did not improve from 0.89294\n","365/365 [==============================] - 9s 25ms/step - loss: 0.0106 - acc: 0.9967 - acc_combo: 0.9971 - val_loss: 0.5619 - val_acc: 0.8718 - val_acc_combo: 0.8918 - lr: 6.2500e-04\n","Epoch 46/100\n","364/365 [============================>.] - ETA: 0s - loss: 0.0099 - acc: 0.9972 - acc_combo: 0.9977\n","Epoch 00046: val_acc_combo did not improve from 0.89294\n","365/365 [==============================] - 9s 25ms/step - loss: 0.0100 - acc: 0.9972 - acc_combo: 0.9977 - val_loss: 0.5683 - val_acc: 0.8725 - val_acc_combo: 0.8925 - lr: 6.2500e-04\n","Epoch 47/100\n","365/365 [==============================] - ETA: 0s - loss: 0.0075 - acc: 0.9981 - acc_combo: 0.9984\n","Epoch 00047: val_acc_combo improved from 0.89294 to 0.89578, saving model to data/fold0.h5\n","365/365 [==============================] - 10s 28ms/step - loss: 0.0075 - acc: 0.9981 - acc_combo: 0.9984 - val_loss: 0.5625 - val_acc: 0.8773 - val_acc_combo: 0.8958 - lr: 6.2500e-04\n","Epoch 48/100\n","297/365 [=======================>......] - ETA: 1s - loss: 0.0083 - acc: 0.9980 - acc_combo: 0.9983"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-61-a6e9bd5eb606>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m               \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0myy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_stat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0myy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0myy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m               callbacks=[plateau, early_stopping, checkpoint])\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mval_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"JIjRLwGG0dRp","colab_type":"code","pycharm":{},"colab":{}},"source":["proba_y = np.zeros((len(x), 19))\n","proba_y=model.predict([x,train_stat], verbose=1, batch_size=1024)\n","proba_y=np.argmax(proba_y,axis=1)\n","#print(proba_y)\n","def acc_combo(y, y_pred):\n","    # 数值ID与行为编码的对应关系\n","    mapping = {0: 'A_0', 1: 'A_1', 2: 'A_2', 3: 'A_3', \n","        4: 'D_4', 5: 'A_5', 6: 'B_1',7: 'B_5', \n","        8: 'B_2', 9: 'B_3', 10: 'B_0', 11: 'A_6', \n","        12: 'C_1', 13: 'C_3', 14: 'C_0', 15: 'B_6', \n","        16: 'C_2', 17: 'C_5', 18: 'C_6'}\n","    # 将行为ID转为编码\n","    code_y, code_y_pred = mapping[y], mapping[y_pred]\n","    if code_y == code_y_pred: #编码完全相同得分1.0\n","        return 1.0\n","    elif code_y.split(\"_\")[0] == code_y_pred.split(\"_\")[0]: #编码仅字母部分相同得分1.0/7\n","        return 1.0/7\n","    elif code_y.split(\"_\")[1] == code_y_pred.split(\"_\")[1]: #编码仅数字部分相同得分1.0/3\n","        return 1.0/3\n","    else:\n","        return 0.0\n","score = sum(acc_combo(y_true, y_pred) for y_true, y_pred in zip(y, proba_y)) / proba_y.shape[0]\n","print(round(score, 5))\n","\n","sub.behavior_id=np.argmax(proba_t, axis=1)\n","print(sub)\n","from datetime import *\n","current = datetime.now()\n","current=current.strftime('%m-%d-%H-%M')\n","sub.to_csv('data/%s_sub%.5f.csv' %(current,score), index=False)\n","#生成半监督训练数据\n","sub[\"proba\"]=np.max(proba_t,axis=1)\n","sub.to_csv(\"data/semi_test_8_nobest.csv\",index=False)\n","sub.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T_nMWK6NsSFT","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}